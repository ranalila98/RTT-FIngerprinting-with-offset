{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "832b9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "968fd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining no of csv file into one file with name combine_csv\n",
    "os.chdir('D:\\Final')\n",
    "extension='csv'\n",
    "\n",
    "all_filenames=[i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "combined_csv=pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "df=combined_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9255c037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP1RTT</th>\n",
       "      <th>AP1STDEV</th>\n",
       "      <th>AP2RTT</th>\n",
       "      <th>AP2STDEV</th>\n",
       "      <th>AP3RTT</th>\n",
       "      <th>AP3STDEV</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.789</td>\n",
       "      <td>0.371</td>\n",
       "      <td>1.311</td>\n",
       "      <td>0.082</td>\n",
       "      <td>11.037</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.731</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.186</td>\n",
       "      <td>10.452</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.731</td>\n",
       "      <td>0.442</td>\n",
       "      <td>1.155</td>\n",
       "      <td>0.171</td>\n",
       "      <td>9.368</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.672</td>\n",
       "      <td>0.698</td>\n",
       "      <td>1.194</td>\n",
       "      <td>0.206</td>\n",
       "      <td>9.709</td>\n",
       "      <td>1.533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.643</td>\n",
       "      <td>2.609</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.151</td>\n",
       "      <td>9.748</td>\n",
       "      <td>2.350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>785</th>\n",
       "      <td>7.320</td>\n",
       "      <td>0.305</td>\n",
       "      <td>11.829</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.943</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>7.242</td>\n",
       "      <td>0.312</td>\n",
       "      <td>11.819</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.967</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>787</th>\n",
       "      <td>7.144</td>\n",
       "      <td>0.406</td>\n",
       "      <td>11.887</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.930</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>7.164</td>\n",
       "      <td>0.400</td>\n",
       "      <td>11.829</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.995</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>6.930</td>\n",
       "      <td>0.453</td>\n",
       "      <td>11.780</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.978</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28010 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AP1RTT  AP1STDEV  AP2RTT  AP2STDEV  AP3RTT  AP3STDEV  x  y\n",
       "0     7.789     0.371   1.311     0.082  11.037     1.192  1  1\n",
       "1     7.731     0.277   0.999     0.186  10.452     0.256  1  1\n",
       "2     7.731     0.442   1.155     0.171   9.368     0.661  1  1\n",
       "3     7.672     0.698   1.194     0.206   9.709     1.533  1  1\n",
       "4     7.643     2.609   1.038     0.151   9.748     2.350  1  1\n",
       "..      ...       ...     ...       ...     ...       ... .. ..\n",
       "785   7.320     0.305  11.829     0.122   0.530     0.943  6  8\n",
       "786   7.242     0.312  11.819     0.161   0.412     0.967  6  8\n",
       "787   7.144     0.406  11.887     0.130   0.530     0.930  6  8\n",
       "788   7.164     0.400  11.829     0.100   0.491     0.995  6  8\n",
       "789   6.930     0.453  11.780     0.065   0.491     0.978  6  8\n",
       "\n",
       "[28010 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(['AP1RSS','AP2RSS','AP3RSS'],axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc96b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>A_P_1_R_T_T</th>\n",
       "      <th>A_P_1_S_T_D_E_V</th>\n",
       "      <th>A_P_2_R_T_T</th>\n",
       "      <th>A_P_2_S_T_D_E_V</th>\n",
       "      <th>A_P_3_R_T_T</th>\n",
       "      <th>A_P_3_S_T_D_E_V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.682077</td>\n",
       "      <td>1.468045</td>\n",
       "      <td>1.050224</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>10.568485</td>\n",
       "      <td>1.682911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.306811</td>\n",
       "      <td>1.604308</td>\n",
       "      <td>2.091507</td>\n",
       "      <td>1.572285</td>\n",
       "      <td>9.860562</td>\n",
       "      <td>0.866540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.566168</td>\n",
       "      <td>1.096188</td>\n",
       "      <td>2.942460</td>\n",
       "      <td>0.308641</td>\n",
       "      <td>8.756523</td>\n",
       "      <td>0.427450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.209767</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>2.970219</td>\n",
       "      <td>0.294924</td>\n",
       "      <td>11.209696</td>\n",
       "      <td>0.226628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.810318</td>\n",
       "      <td>0.626128</td>\n",
       "      <td>4.366355</td>\n",
       "      <td>0.568405</td>\n",
       "      <td>7.101791</td>\n",
       "      <td>1.755211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.948261</td>\n",
       "      <td>0.485397</td>\n",
       "      <td>7.459974</td>\n",
       "      <td>0.139654</td>\n",
       "      <td>6.090467</td>\n",
       "      <td>0.277205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.669858</td>\n",
       "      <td>2.234651</td>\n",
       "      <td>6.657000</td>\n",
       "      <td>1.608237</td>\n",
       "      <td>7.164004</td>\n",
       "      <td>0.931583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.172300</td>\n",
       "      <td>0.267215</td>\n",
       "      <td>10.099617</td>\n",
       "      <td>0.161544</td>\n",
       "      <td>5.680480</td>\n",
       "      <td>1.060779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.961054</td>\n",
       "      <td>0.170011</td>\n",
       "      <td>2.474627</td>\n",
       "      <td>0.931758</td>\n",
       "      <td>10.461686</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.596997</td>\n",
       "      <td>1.857211</td>\n",
       "      <td>3.443035</td>\n",
       "      <td>0.201802</td>\n",
       "      <td>9.061454</td>\n",
       "      <td>1.517238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.019883</td>\n",
       "      <td>0.362749</td>\n",
       "      <td>3.758254</td>\n",
       "      <td>0.120847</td>\n",
       "      <td>7.664262</td>\n",
       "      <td>0.270639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.431853</td>\n",
       "      <td>0.698381</td>\n",
       "      <td>5.432772</td>\n",
       "      <td>0.808957</td>\n",
       "      <td>7.630549</td>\n",
       "      <td>0.938381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.300867</td>\n",
       "      <td>0.442419</td>\n",
       "      <td>5.085693</td>\n",
       "      <td>1.357892</td>\n",
       "      <td>6.848202</td>\n",
       "      <td>1.069756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.669894</td>\n",
       "      <td>0.733214</td>\n",
       "      <td>5.856901</td>\n",
       "      <td>0.552265</td>\n",
       "      <td>4.424874</td>\n",
       "      <td>0.736379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4.077596</td>\n",
       "      <td>0.757938</td>\n",
       "      <td>6.953378</td>\n",
       "      <td>0.566308</td>\n",
       "      <td>5.810792</td>\n",
       "      <td>0.619053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4.339335</td>\n",
       "      <td>0.303702</td>\n",
       "      <td>8.952600</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>4.561169</td>\n",
       "      <td>1.029254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.132744</td>\n",
       "      <td>1.341447</td>\n",
       "      <td>3.648221</td>\n",
       "      <td>1.183408</td>\n",
       "      <td>9.684984</td>\n",
       "      <td>0.763672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7.262630</td>\n",
       "      <td>1.168084</td>\n",
       "      <td>3.710332</td>\n",
       "      <td>0.311626</td>\n",
       "      <td>8.444904</td>\n",
       "      <td>0.546937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.320111</td>\n",
       "      <td>0.512686</td>\n",
       "      <td>4.073060</td>\n",
       "      <td>0.435144</td>\n",
       "      <td>7.792985</td>\n",
       "      <td>0.207471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.979253</td>\n",
       "      <td>0.891789</td>\n",
       "      <td>4.989121</td>\n",
       "      <td>0.924127</td>\n",
       "      <td>6.681760</td>\n",
       "      <td>2.124405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3.664835</td>\n",
       "      <td>0.914851</td>\n",
       "      <td>8.717008</td>\n",
       "      <td>1.680310</td>\n",
       "      <td>4.559160</td>\n",
       "      <td>0.353236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10.094730</td>\n",
       "      <td>0.819869</td>\n",
       "      <td>5.704038</td>\n",
       "      <td>1.900488</td>\n",
       "      <td>11.633838</td>\n",
       "      <td>1.324471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.670010</td>\n",
       "      <td>1.196257</td>\n",
       "      <td>4.847702</td>\n",
       "      <td>0.768664</td>\n",
       "      <td>7.913525</td>\n",
       "      <td>1.138885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.963262</td>\n",
       "      <td>1.277812</td>\n",
       "      <td>5.094497</td>\n",
       "      <td>1.900971</td>\n",
       "      <td>9.542974</td>\n",
       "      <td>0.222113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.247958</td>\n",
       "      <td>1.967185</td>\n",
       "      <td>7.742387</td>\n",
       "      <td>0.644140</td>\n",
       "      <td>7.543527</td>\n",
       "      <td>0.299946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.285183</td>\n",
       "      <td>0.745035</td>\n",
       "      <td>6.695376</td>\n",
       "      <td>0.193696</td>\n",
       "      <td>5.381711</td>\n",
       "      <td>1.098018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.712279</td>\n",
       "      <td>1.139860</td>\n",
       "      <td>6.940536</td>\n",
       "      <td>0.311796</td>\n",
       "      <td>3.564541</td>\n",
       "      <td>0.808598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6.395618</td>\n",
       "      <td>0.319665</td>\n",
       "      <td>7.800133</td>\n",
       "      <td>0.267582</td>\n",
       "      <td>3.357849</td>\n",
       "      <td>0.331727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7.595290</td>\n",
       "      <td>1.048518</td>\n",
       "      <td>11.264811</td>\n",
       "      <td>0.301927</td>\n",
       "      <td>3.884758</td>\n",
       "      <td>0.649658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11.408504</td>\n",
       "      <td>0.714262</td>\n",
       "      <td>5.881772</td>\n",
       "      <td>1.246364</td>\n",
       "      <td>8.554079</td>\n",
       "      <td>0.609167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9.316401</td>\n",
       "      <td>0.822399</td>\n",
       "      <td>5.554277</td>\n",
       "      <td>0.216622</td>\n",
       "      <td>9.118129</td>\n",
       "      <td>0.924308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9.202826</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>5.802463</td>\n",
       "      <td>1.513773</td>\n",
       "      <td>7.514950</td>\n",
       "      <td>1.536497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6.341018</td>\n",
       "      <td>0.627516</td>\n",
       "      <td>7.910018</td>\n",
       "      <td>0.727868</td>\n",
       "      <td>2.725399</td>\n",
       "      <td>0.202878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8.434157</td>\n",
       "      <td>0.458550</td>\n",
       "      <td>9.463184</td>\n",
       "      <td>0.145792</td>\n",
       "      <td>1.790935</td>\n",
       "      <td>1.399032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.178918</td>\n",
       "      <td>1.066333</td>\n",
       "      <td>5.965107</td>\n",
       "      <td>0.141504</td>\n",
       "      <td>9.408819</td>\n",
       "      <td>0.133744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10.418660</td>\n",
       "      <td>0.407708</td>\n",
       "      <td>7.160464</td>\n",
       "      <td>0.486028</td>\n",
       "      <td>9.887784</td>\n",
       "      <td>0.460747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9.312207</td>\n",
       "      <td>0.322244</td>\n",
       "      <td>9.164037</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>7.157308</td>\n",
       "      <td>0.404662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5.741123</td>\n",
       "      <td>0.888978</td>\n",
       "      <td>9.734936</td>\n",
       "      <td>0.873201</td>\n",
       "      <td>1.912884</td>\n",
       "      <td>0.148901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7.183425</td>\n",
       "      <td>0.473028</td>\n",
       "      <td>11.232511</td>\n",
       "      <td>0.330619</td>\n",
       "      <td>0.476505</td>\n",
       "      <td>0.981901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x  y  A_P_1_R_T_T  A_P_1_S_T_D_E_V  A_P_2_R_T_T  A_P_2_S_T_D_E_V  \\\n",
       "0   1  1     7.682077         1.468045     1.050224         0.138249   \n",
       "1   1  2     7.306811         1.604308     2.091507         1.572285   \n",
       "2   1  3     5.566168         1.096188     2.942460         0.308641   \n",
       "3   1  4     5.209767         0.518800     2.970219         0.294924   \n",
       "4   1  5     3.810318         0.626128     4.366355         0.568405   \n",
       "5   1  6     4.948261         0.485397     7.459974         0.139654   \n",
       "6   1  7     2.669858         2.234651     6.657000         1.608237   \n",
       "7   1  8     1.172300         0.267215    10.099617         0.161544   \n",
       "8   2  1     7.961054         0.170011     2.474627         0.931758   \n",
       "9   2  2     6.596997         1.857211     3.443035         0.201802   \n",
       "10  2  3     6.019883         0.362749     3.758254         0.120847   \n",
       "11  2  4     4.431853         0.698381     5.432772         0.808957   \n",
       "12  2  5     4.300867         0.442419     5.085693         1.357892   \n",
       "13  2  6     3.669894         0.733214     5.856901         0.552265   \n",
       "14  2  7     4.077596         0.757938     6.953378         0.566308   \n",
       "15  2  8     4.339335         0.303702     8.952600         0.258842   \n",
       "16  3  1     8.132744         1.341447     3.648221         1.183408   \n",
       "17  3  2     7.262630         1.168084     3.710332         0.311626   \n",
       "18  3  3     7.320111         0.512686     4.073060         0.435144   \n",
       "19  3  4     4.979253         0.891789     4.989121         0.924127   \n",
       "20  3  8     3.664835         0.914851     8.717008         1.680310   \n",
       "21  4  1    10.094730         0.819869     5.704038         1.900488   \n",
       "22  4  2     7.670010         1.196257     4.847702         0.768664   \n",
       "23  4  3     6.963262         1.277812     5.094497         1.900971   \n",
       "24  4  4     7.247958         1.967185     7.742387         0.644140   \n",
       "25  4  5     5.285183         0.745035     6.695376         0.193696   \n",
       "26  4  6     5.712279         1.139860     6.940536         0.311796   \n",
       "27  4  7     6.395618         0.319665     7.800133         0.267582   \n",
       "28  4  8     7.595290         1.048518    11.264811         0.301927   \n",
       "29  5  1    11.408504         0.714262     5.881772         1.246364   \n",
       "30  5  2     9.316401         0.822399     5.554277         0.216622   \n",
       "31  5  3     9.202826         0.650289     5.802463         1.513773   \n",
       "32  5  7     6.341018         0.627516     7.910018         0.727868   \n",
       "33  5  8     8.434157         0.458550     9.463184         0.145792   \n",
       "34  6  1    10.178918         1.066333     5.965107         0.141504   \n",
       "35  6  2    10.418660         0.407708     7.160464         0.486028   \n",
       "36  6  3     9.312207         0.322244     9.164037         0.787931   \n",
       "37  6  7     5.741123         0.888978     9.734936         0.873201   \n",
       "38  6  8     7.183425         0.473028    11.232511         0.330619   \n",
       "\n",
       "    A_P_3_R_T_T  A_P_3_S_T_D_E_V  \n",
       "0     10.568485         1.682911  \n",
       "1      9.860562         0.866540  \n",
       "2      8.756523         0.427450  \n",
       "3     11.209696         0.226628  \n",
       "4      7.101791         1.755211  \n",
       "5      6.090467         0.277205  \n",
       "6      7.164004         0.931583  \n",
       "7      5.680480         1.060779  \n",
       "8     10.461686         0.759400  \n",
       "9      9.061454         1.517238  \n",
       "10     7.664262         0.270639  \n",
       "11     7.630549         0.938381  \n",
       "12     6.848202         1.069756  \n",
       "13     4.424874         0.736379  \n",
       "14     5.810792         0.619053  \n",
       "15     4.561169         1.029254  \n",
       "16     9.684984         0.763672  \n",
       "17     8.444904         0.546937  \n",
       "18     7.792985         0.207471  \n",
       "19     6.681760         2.124405  \n",
       "20     4.559160         0.353236  \n",
       "21    11.633838         1.324471  \n",
       "22     7.913525         1.138885  \n",
       "23     9.542974         0.222113  \n",
       "24     7.543527         0.299946  \n",
       "25     5.381711         1.098018  \n",
       "26     3.564541         0.808598  \n",
       "27     3.357849         0.331727  \n",
       "28     3.884758         0.649658  \n",
       "29     8.554079         0.609167  \n",
       "30     9.118129         0.924308  \n",
       "31     7.514950         1.536497  \n",
       "32     2.725399         0.202878  \n",
       "33     1.790935         1.399032  \n",
       "34     9.408819         0.133744  \n",
       "35     9.887784         0.460747  \n",
       "36     7.157308         0.404662  \n",
       "37     1.912884         0.148901  \n",
       "38     0.476505         0.981901  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the data by 'x' and 'y', and calculate the minimum, maximum, 25th, 50th, and 75th percentiles for each column for local feature extractions\n",
    "#grouped = df.groupby(['x', 'y']).agg(['min'])\n",
    "\n",
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped = df.groupby(['x', 'y']).mean()\n",
    "#grouped = pd.concat([grouped, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "df = grouped.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd7d45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>AP1RTT_MEAN</th>\n",
       "      <th>AP1STDEV_MEAN</th>\n",
       "      <th>AP2RTT_MEAN</th>\n",
       "      <th>AP1STDEV_MEAN</th>\n",
       "      <th>AP3RTT_MEAN</th>\n",
       "      <th>AP1STDEV_MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.682077</td>\n",
       "      <td>1.468045</td>\n",
       "      <td>1.050224</td>\n",
       "      <td>0.138249</td>\n",
       "      <td>10.568485</td>\n",
       "      <td>1.682911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.306811</td>\n",
       "      <td>1.604308</td>\n",
       "      <td>2.091507</td>\n",
       "      <td>1.572285</td>\n",
       "      <td>9.860562</td>\n",
       "      <td>0.866540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.566168</td>\n",
       "      <td>1.096188</td>\n",
       "      <td>2.942460</td>\n",
       "      <td>0.308641</td>\n",
       "      <td>8.756523</td>\n",
       "      <td>0.427450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.209767</td>\n",
       "      <td>0.518800</td>\n",
       "      <td>2.970219</td>\n",
       "      <td>0.294924</td>\n",
       "      <td>11.209696</td>\n",
       "      <td>0.226628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.810318</td>\n",
       "      <td>0.626128</td>\n",
       "      <td>4.366355</td>\n",
       "      <td>0.568405</td>\n",
       "      <td>7.101791</td>\n",
       "      <td>1.755211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.948261</td>\n",
       "      <td>0.485397</td>\n",
       "      <td>7.459974</td>\n",
       "      <td>0.139654</td>\n",
       "      <td>6.090467</td>\n",
       "      <td>0.277205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.669858</td>\n",
       "      <td>2.234651</td>\n",
       "      <td>6.657000</td>\n",
       "      <td>1.608237</td>\n",
       "      <td>7.164004</td>\n",
       "      <td>0.931583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.172300</td>\n",
       "      <td>0.267215</td>\n",
       "      <td>10.099617</td>\n",
       "      <td>0.161544</td>\n",
       "      <td>5.680480</td>\n",
       "      <td>1.060779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.961054</td>\n",
       "      <td>0.170011</td>\n",
       "      <td>2.474627</td>\n",
       "      <td>0.931758</td>\n",
       "      <td>10.461686</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.596997</td>\n",
       "      <td>1.857211</td>\n",
       "      <td>3.443035</td>\n",
       "      <td>0.201802</td>\n",
       "      <td>9.061454</td>\n",
       "      <td>1.517238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6.019883</td>\n",
       "      <td>0.362749</td>\n",
       "      <td>3.758254</td>\n",
       "      <td>0.120847</td>\n",
       "      <td>7.664262</td>\n",
       "      <td>0.270639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.431853</td>\n",
       "      <td>0.698381</td>\n",
       "      <td>5.432772</td>\n",
       "      <td>0.808957</td>\n",
       "      <td>7.630549</td>\n",
       "      <td>0.938381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.300867</td>\n",
       "      <td>0.442419</td>\n",
       "      <td>5.085693</td>\n",
       "      <td>1.357892</td>\n",
       "      <td>6.848202</td>\n",
       "      <td>1.069756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.669894</td>\n",
       "      <td>0.733214</td>\n",
       "      <td>5.856901</td>\n",
       "      <td>0.552265</td>\n",
       "      <td>4.424874</td>\n",
       "      <td>0.736379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4.077596</td>\n",
       "      <td>0.757938</td>\n",
       "      <td>6.953378</td>\n",
       "      <td>0.566308</td>\n",
       "      <td>5.810792</td>\n",
       "      <td>0.619053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4.339335</td>\n",
       "      <td>0.303702</td>\n",
       "      <td>8.952600</td>\n",
       "      <td>0.258842</td>\n",
       "      <td>4.561169</td>\n",
       "      <td>1.029254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.132744</td>\n",
       "      <td>1.341447</td>\n",
       "      <td>3.648221</td>\n",
       "      <td>1.183408</td>\n",
       "      <td>9.684984</td>\n",
       "      <td>0.763672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7.262630</td>\n",
       "      <td>1.168084</td>\n",
       "      <td>3.710332</td>\n",
       "      <td>0.311626</td>\n",
       "      <td>8.444904</td>\n",
       "      <td>0.546937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.320111</td>\n",
       "      <td>0.512686</td>\n",
       "      <td>4.073060</td>\n",
       "      <td>0.435144</td>\n",
       "      <td>7.792985</td>\n",
       "      <td>0.207471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.979253</td>\n",
       "      <td>0.891789</td>\n",
       "      <td>4.989121</td>\n",
       "      <td>0.924127</td>\n",
       "      <td>6.681760</td>\n",
       "      <td>2.124405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3.664835</td>\n",
       "      <td>0.914851</td>\n",
       "      <td>8.717008</td>\n",
       "      <td>1.680310</td>\n",
       "      <td>4.559160</td>\n",
       "      <td>0.353236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>10.094730</td>\n",
       "      <td>0.819869</td>\n",
       "      <td>5.704038</td>\n",
       "      <td>1.900488</td>\n",
       "      <td>11.633838</td>\n",
       "      <td>1.324471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.670010</td>\n",
       "      <td>1.196257</td>\n",
       "      <td>4.847702</td>\n",
       "      <td>0.768664</td>\n",
       "      <td>7.913525</td>\n",
       "      <td>1.138885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.963262</td>\n",
       "      <td>1.277812</td>\n",
       "      <td>5.094497</td>\n",
       "      <td>1.900971</td>\n",
       "      <td>9.542974</td>\n",
       "      <td>0.222113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.247958</td>\n",
       "      <td>1.967185</td>\n",
       "      <td>7.742387</td>\n",
       "      <td>0.644140</td>\n",
       "      <td>7.543527</td>\n",
       "      <td>0.299946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.285183</td>\n",
       "      <td>0.745035</td>\n",
       "      <td>6.695376</td>\n",
       "      <td>0.193696</td>\n",
       "      <td>5.381711</td>\n",
       "      <td>1.098018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.712279</td>\n",
       "      <td>1.139860</td>\n",
       "      <td>6.940536</td>\n",
       "      <td>0.311796</td>\n",
       "      <td>3.564541</td>\n",
       "      <td>0.808598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6.395618</td>\n",
       "      <td>0.319665</td>\n",
       "      <td>7.800133</td>\n",
       "      <td>0.267582</td>\n",
       "      <td>3.357849</td>\n",
       "      <td>0.331727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7.595290</td>\n",
       "      <td>1.048518</td>\n",
       "      <td>11.264811</td>\n",
       "      <td>0.301927</td>\n",
       "      <td>3.884758</td>\n",
       "      <td>0.649658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11.408504</td>\n",
       "      <td>0.714262</td>\n",
       "      <td>5.881772</td>\n",
       "      <td>1.246364</td>\n",
       "      <td>8.554079</td>\n",
       "      <td>0.609167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9.316401</td>\n",
       "      <td>0.822399</td>\n",
       "      <td>5.554277</td>\n",
       "      <td>0.216622</td>\n",
       "      <td>9.118129</td>\n",
       "      <td>0.924308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9.202826</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>5.802463</td>\n",
       "      <td>1.513773</td>\n",
       "      <td>7.514950</td>\n",
       "      <td>1.536497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6.341018</td>\n",
       "      <td>0.627516</td>\n",
       "      <td>7.910018</td>\n",
       "      <td>0.727868</td>\n",
       "      <td>2.725399</td>\n",
       "      <td>0.202878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8.434157</td>\n",
       "      <td>0.458550</td>\n",
       "      <td>9.463184</td>\n",
       "      <td>0.145792</td>\n",
       "      <td>1.790935</td>\n",
       "      <td>1.399032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>10.178918</td>\n",
       "      <td>1.066333</td>\n",
       "      <td>5.965107</td>\n",
       "      <td>0.141504</td>\n",
       "      <td>9.408819</td>\n",
       "      <td>0.133744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10.418660</td>\n",
       "      <td>0.407708</td>\n",
       "      <td>7.160464</td>\n",
       "      <td>0.486028</td>\n",
       "      <td>9.887784</td>\n",
       "      <td>0.460747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9.312207</td>\n",
       "      <td>0.322244</td>\n",
       "      <td>9.164037</td>\n",
       "      <td>0.787931</td>\n",
       "      <td>7.157308</td>\n",
       "      <td>0.404662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5.741123</td>\n",
       "      <td>0.888978</td>\n",
       "      <td>9.734936</td>\n",
       "      <td>0.873201</td>\n",
       "      <td>1.912884</td>\n",
       "      <td>0.148901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7.183425</td>\n",
       "      <td>0.473028</td>\n",
       "      <td>11.232511</td>\n",
       "      <td>0.330619</td>\n",
       "      <td>0.476505</td>\n",
       "      <td>0.981901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x  y  AP1RTT_MEAN  AP1STDEV_MEAN  AP2RTT_MEAN  AP1STDEV_MEAN  AP3RTT_MEAN  \\\n",
       "0   1  1     7.682077       1.468045     1.050224       0.138249    10.568485   \n",
       "1   1  2     7.306811       1.604308     2.091507       1.572285     9.860562   \n",
       "2   1  3     5.566168       1.096188     2.942460       0.308641     8.756523   \n",
       "3   1  4     5.209767       0.518800     2.970219       0.294924    11.209696   \n",
       "4   1  5     3.810318       0.626128     4.366355       0.568405     7.101791   \n",
       "5   1  6     4.948261       0.485397     7.459974       0.139654     6.090467   \n",
       "6   1  7     2.669858       2.234651     6.657000       1.608237     7.164004   \n",
       "7   1  8     1.172300       0.267215    10.099617       0.161544     5.680480   \n",
       "8   2  1     7.961054       0.170011     2.474627       0.931758    10.461686   \n",
       "9   2  2     6.596997       1.857211     3.443035       0.201802     9.061454   \n",
       "10  2  3     6.019883       0.362749     3.758254       0.120847     7.664262   \n",
       "11  2  4     4.431853       0.698381     5.432772       0.808957     7.630549   \n",
       "12  2  5     4.300867       0.442419     5.085693       1.357892     6.848202   \n",
       "13  2  6     3.669894       0.733214     5.856901       0.552265     4.424874   \n",
       "14  2  7     4.077596       0.757938     6.953378       0.566308     5.810792   \n",
       "15  2  8     4.339335       0.303702     8.952600       0.258842     4.561169   \n",
       "16  3  1     8.132744       1.341447     3.648221       1.183408     9.684984   \n",
       "17  3  2     7.262630       1.168084     3.710332       0.311626     8.444904   \n",
       "18  3  3     7.320111       0.512686     4.073060       0.435144     7.792985   \n",
       "19  3  4     4.979253       0.891789     4.989121       0.924127     6.681760   \n",
       "20  3  8     3.664835       0.914851     8.717008       1.680310     4.559160   \n",
       "21  4  1    10.094730       0.819869     5.704038       1.900488    11.633838   \n",
       "22  4  2     7.670010       1.196257     4.847702       0.768664     7.913525   \n",
       "23  4  3     6.963262       1.277812     5.094497       1.900971     9.542974   \n",
       "24  4  4     7.247958       1.967185     7.742387       0.644140     7.543527   \n",
       "25  4  5     5.285183       0.745035     6.695376       0.193696     5.381711   \n",
       "26  4  6     5.712279       1.139860     6.940536       0.311796     3.564541   \n",
       "27  4  7     6.395618       0.319665     7.800133       0.267582     3.357849   \n",
       "28  4  8     7.595290       1.048518    11.264811       0.301927     3.884758   \n",
       "29  5  1    11.408504       0.714262     5.881772       1.246364     8.554079   \n",
       "30  5  2     9.316401       0.822399     5.554277       0.216622     9.118129   \n",
       "31  5  3     9.202826       0.650289     5.802463       1.513773     7.514950   \n",
       "32  5  7     6.341018       0.627516     7.910018       0.727868     2.725399   \n",
       "33  5  8     8.434157       0.458550     9.463184       0.145792     1.790935   \n",
       "34  6  1    10.178918       1.066333     5.965107       0.141504     9.408819   \n",
       "35  6  2    10.418660       0.407708     7.160464       0.486028     9.887784   \n",
       "36  6  3     9.312207       0.322244     9.164037       0.787931     7.157308   \n",
       "37  6  7     5.741123       0.888978     9.734936       0.873201     1.912884   \n",
       "38  6  8     7.183425       0.473028    11.232511       0.330619     0.476505   \n",
       "\n",
       "    AP1STDEV_MEAN  \n",
       "0        1.682911  \n",
       "1        0.866540  \n",
       "2        0.427450  \n",
       "3        0.226628  \n",
       "4        1.755211  \n",
       "5        0.277205  \n",
       "6        0.931583  \n",
       "7        1.060779  \n",
       "8        0.759400  \n",
       "9        1.517238  \n",
       "10       0.270639  \n",
       "11       0.938381  \n",
       "12       1.069756  \n",
       "13       0.736379  \n",
       "14       0.619053  \n",
       "15       1.029254  \n",
       "16       0.763672  \n",
       "17       0.546937  \n",
       "18       0.207471  \n",
       "19       2.124405  \n",
       "20       0.353236  \n",
       "21       1.324471  \n",
       "22       1.138885  \n",
       "23       0.222113  \n",
       "24       0.299946  \n",
       "25       1.098018  \n",
       "26       0.808598  \n",
       "27       0.331727  \n",
       "28       0.649658  \n",
       "29       0.609167  \n",
       "30       0.924308  \n",
       "31       1.536497  \n",
       "32       0.202878  \n",
       "33       1.399032  \n",
       "34       0.133744  \n",
       "35       0.460747  \n",
       "36       0.404662  \n",
       "37       0.148901  \n",
       "38       0.981901  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_name={\n",
    "          'A_P_1_R_T_T':'AP1RTT_MEAN','A_P_2_R_T_T':'AP2RTT_MEAN','A_P_3_R_T_T':'AP3RTT_MEAN',\n",
    "          'A_P_1_R_S_S':'AP1RSS_MEAN', 'A_P_2_R_S_S':'AP2RSS_MEAN', 'A_P_3_R_S_S':'AP3RSS_MEAN',\n",
    "          'A_P_1_S_T_D_E_V':'AP1STDEV_MEAN','A_P_2_S_T_D_E_V':'AP1STDEV_MEAN','A_P_3_S_T_D_E_V':'AP1STDEV_MEAN'}\n",
    "df.rename(columns=new_name,inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d61de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=df.iloc[:,2:] \n",
    "output_data = df.iloc[:, :2]\n",
    "input_data=np.array(input_data.values)\n",
    "output_data=np.array(output_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e216af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 2)\n",
      "(39, 6)\n"
     ]
    }
   ],
   "source": [
    "X=input_data\n",
    "y=output_data\n",
    "print(output_data.shape)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5aadc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d65b158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3676523d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 6, 'n_estimators': 200}\n",
      "Mean Squared Error in meter: 1.346\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 1.160\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 29.689\n",
      "R2 score is in percent: 68.20\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,RF_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, RF_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1cb842fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 1.35\n",
      "Root Mean Squared Error (RMSE) on new data in m: 1.16\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 36.520\n",
      "R2 score is in percent: 67.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from math import sqrt\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "rmse=sqrt(mean_squared_error(y_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse)) \n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,knn_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d97d5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "# Define the model architecture\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=16), input_shape=(6,), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    for i in range(hp.Int('num_hidden_layers', 1, 10)):\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_' + str(i+2), min_value=32, max_value=512, step=16),\n",
    "                                 activation=hp.Choice('activation_' + str(i+2), values=['relu', 'sigmoid', 'tanh'])))\n",
    "        model.add(keras.layers.Dropout(hp.Float('dropout_' + str(i+2), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "        \n",
    "    model.add(keras.layers.Dense(units=2, activation='linear'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "                        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d1c934c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from test_raww\\hellooooo_raww\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=3,\n",
    "    directory='test_raww',\n",
    "    project_name='hellooooo_raww')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d38f302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=200,\n",
    "           validation_data=(X_test,y_test) ,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128212a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in test_raww\\hellooooo_raww\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x000001B289ABC9A0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 112\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 1\n",
      "units_2: 304\n",
      "activation_2: sigmoid\n",
      "dropout_2: 0.4\n",
      "learning_rate: 0.001\n",
      "units_3: 208\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.1\n",
      "units_4: 336\n",
      "activation_4: tanh\n",
      "dropout_4: 0.2\n",
      "units_5: 32\n",
      "activation_5: sigmoid\n",
      "dropout_5: 0.1\n",
      "units_6: 480\n",
      "activation_6: relu\n",
      "dropout_6: 0.1\n",
      "units_7: 192\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.2\n",
      "units_8: 320\n",
      "activation_8: sigmoid\n",
      "dropout_8: 0.30000000000000004\n",
      "units_9: 208\n",
      "activation_9: tanh\n",
      "dropout_9: 0.30000000000000004\n",
      "units_10: 160\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.0\n",
      "Score: 0.8311991890271505\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 352\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 1\n",
      "units_2: 336\n",
      "activation_2: tanh\n",
      "dropout_2: 0.4\n",
      "learning_rate: 0.01\n",
      "units_3: 352\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.1\n",
      "units_4: 400\n",
      "activation_4: relu\n",
      "dropout_4: 0.2\n",
      "units_5: 352\n",
      "activation_5: sigmoid\n",
      "dropout_5: 0.4\n",
      "units_6: 208\n",
      "activation_6: relu\n",
      "dropout_6: 0.30000000000000004\n",
      "units_7: 224\n",
      "activation_7: relu\n",
      "dropout_7: 0.30000000000000004\n",
      "units_8: 352\n",
      "activation_8: relu\n",
      "dropout_8: 0.2\n",
      "units_9: 432\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.2\n",
      "units_10: 256\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.1\n",
      "Score: 1.3457144896189372\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 432\n",
      "dropout_1: 0.2\n",
      "num_hidden_layers: 2\n",
      "units_2: 304\n",
      "activation_2: relu\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.001\n",
      "units_3: 192\n",
      "activation_3: relu\n",
      "dropout_3: 0.0\n",
      "units_4: 320\n",
      "activation_4: relu\n",
      "dropout_4: 0.1\n",
      "units_5: 144\n",
      "activation_5: tanh\n",
      "dropout_5: 0.0\n",
      "units_6: 240\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.1\n",
      "units_7: 336\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.2\n",
      "units_8: 320\n",
      "activation_8: relu\n",
      "dropout_8: 0.4\n",
      "units_9: 128\n",
      "activation_9: relu\n",
      "dropout_9: 0.2\n",
      "units_10: 432\n",
      "activation_10: relu\n",
      "dropout_10: 0.2\n",
      "Score: 1.4020013411839802\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 400\n",
      "dropout_1: 0.4\n",
      "num_hidden_layers: 4\n",
      "units_2: 384\n",
      "activation_2: relu\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.001\n",
      "units_3: 400\n",
      "activation_3: tanh\n",
      "dropout_3: 0.0\n",
      "units_4: 64\n",
      "activation_4: sigmoid\n",
      "dropout_4: 0.2\n",
      "units_5: 112\n",
      "activation_5: relu\n",
      "dropout_5: 0.0\n",
      "units_6: 160\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.0\n",
      "units_7: 176\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.0\n",
      "units_8: 96\n",
      "activation_8: sigmoid\n",
      "dropout_8: 0.0\n",
      "units_9: 144\n",
      "activation_9: tanh\n",
      "dropout_9: 0.4\n",
      "units_10: 400\n",
      "activation_10: relu\n",
      "dropout_10: 0.0\n",
      "units_11: 160\n",
      "activation_11: relu\n",
      "dropout_11: 0.4\n",
      "Score: 1.6022570530573528\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 192\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 3\n",
      "units_2: 352\n",
      "activation_2: tanh\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.001\n",
      "units_3: 128\n",
      "activation_3: tanh\n",
      "dropout_3: 0.0\n",
      "units_4: 384\n",
      "activation_4: relu\n",
      "dropout_4: 0.0\n",
      "units_5: 48\n",
      "activation_5: relu\n",
      "dropout_5: 0.1\n",
      "units_6: 432\n",
      "activation_6: tanh\n",
      "dropout_6: 0.1\n",
      "units_7: 64\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.4\n",
      "units_8: 400\n",
      "activation_8: tanh\n",
      "dropout_8: 0.1\n",
      "units_9: 208\n",
      "activation_9: tanh\n",
      "dropout_9: 0.2\n",
      "units_10: 192\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.1\n",
      "Score: 1.70622452100118\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 256\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 4\n",
      "units_2: 240\n",
      "activation_2: tanh\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.0001\n",
      "units_3: 416\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.2\n",
      "units_4: 192\n",
      "activation_4: relu\n",
      "dropout_4: 0.1\n",
      "units_5: 336\n",
      "activation_5: sigmoid\n",
      "dropout_5: 0.4\n",
      "units_6: 240\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.0\n",
      "units_7: 48\n",
      "activation_7: relu\n",
      "dropout_7: 0.4\n",
      "units_8: 96\n",
      "activation_8: tanh\n",
      "dropout_8: 0.2\n",
      "units_9: 48\n",
      "activation_9: tanh\n",
      "dropout_9: 0.2\n",
      "units_10: 128\n",
      "activation_10: relu\n",
      "dropout_10: 0.2\n",
      "Score: 1.8208367029825847\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 448\n",
      "dropout_1: 0.0\n",
      "num_hidden_layers: 5\n",
      "units_2: 256\n",
      "activation_2: relu\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.001\n",
      "units_3: 144\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.4\n",
      "units_4: 64\n",
      "activation_4: tanh\n",
      "dropout_4: 0.4\n",
      "units_5: 464\n",
      "activation_5: relu\n",
      "dropout_5: 0.2\n",
      "units_6: 32\n",
      "activation_6: tanh\n",
      "dropout_6: 0.2\n",
      "units_7: 208\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.1\n",
      "units_8: 224\n",
      "activation_8: relu\n",
      "dropout_8: 0.30000000000000004\n",
      "units_9: 176\n",
      "activation_9: relu\n",
      "dropout_9: 0.30000000000000004\n",
      "units_10: 208\n",
      "activation_10: relu\n",
      "dropout_10: 0.30000000000000004\n",
      "Score: 3.88021190961202\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 48\n",
      "dropout_1: 0.4\n",
      "num_hidden_layers: 4\n",
      "units_2: 176\n",
      "activation_2: sigmoid\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.001\n",
      "units_3: 384\n",
      "activation_3: relu\n",
      "dropout_3: 0.4\n",
      "units_4: 208\n",
      "activation_4: sigmoid\n",
      "dropout_4: 0.4\n",
      "units_5: 128\n",
      "activation_5: relu\n",
      "dropout_5: 0.0\n",
      "units_6: 208\n",
      "activation_6: relu\n",
      "dropout_6: 0.4\n",
      "units_7: 144\n",
      "activation_7: tanh\n",
      "dropout_7: 0.1\n",
      "units_8: 448\n",
      "activation_8: relu\n",
      "dropout_8: 0.4\n",
      "units_9: 480\n",
      "activation_9: relu\n",
      "dropout_9: 0.0\n",
      "units_10: 384\n",
      "activation_10: tanh\n",
      "dropout_10: 0.2\n",
      "Score: 4.11229133605957\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 48\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 8\n",
      "units_2: 128\n",
      "activation_2: tanh\n",
      "dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.01\n",
      "units_3: 304\n",
      "activation_3: relu\n",
      "dropout_3: 0.30000000000000004\n",
      "units_4: 368\n",
      "activation_4: sigmoid\n",
      "dropout_4: 0.30000000000000004\n",
      "units_5: 368\n",
      "activation_5: tanh\n",
      "dropout_5: 0.2\n",
      "units_6: 496\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.30000000000000004\n",
      "units_7: 288\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.1\n",
      "units_8: 112\n",
      "activation_8: tanh\n",
      "dropout_8: 0.30000000000000004\n",
      "units_9: 96\n",
      "activation_9: tanh\n",
      "dropout_9: 0.2\n",
      "units_10: 144\n",
      "activation_10: relu\n",
      "dropout_10: 0.4\n",
      "Score: 4.132053375244141\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 352\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 10\n",
      "units_2: 128\n",
      "activation_2: sigmoid\n",
      "dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "units_3: 496\n",
      "activation_3: relu\n",
      "dropout_3: 0.4\n",
      "units_4: 432\n",
      "activation_4: tanh\n",
      "dropout_4: 0.1\n",
      "units_5: 160\n",
      "activation_5: tanh\n",
      "dropout_5: 0.1\n",
      "units_6: 288\n",
      "activation_6: relu\n",
      "dropout_6: 0.1\n",
      "units_7: 112\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.0\n",
      "units_8: 112\n",
      "activation_8: tanh\n",
      "dropout_8: 0.0\n",
      "units_9: 128\n",
      "activation_9: relu\n",
      "dropout_9: 0.30000000000000004\n",
      "units_10: 160\n",
      "activation_10: tanh\n",
      "dropout_10: 0.4\n",
      "units_11: 32\n",
      "activation_11: relu\n",
      "dropout_11: 0.0\n",
      "Score: 4.160003185272217\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03095779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 15.7331 - mse: 15.7331 - val_loss: 16.2568 - val_mse: 16.2568\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.7029 - mse: 14.7029 - val_loss: 14.8727 - val_mse: 14.8727\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 13.7673 - mse: 13.7673 - val_loss: 13.5851 - val_mse: 13.5851\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 12.3585 - mse: 12.3585 - val_loss: 12.3858 - val_mse: 12.3858\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.7645 - mse: 10.7645 - val_loss: 11.2762 - val_mse: 11.2762\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.4914 - mse: 9.4914 - val_loss: 10.2539 - val_mse: 10.2539\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.8771 - mse: 8.8771 - val_loss: 9.3276 - val_mse: 9.3276\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8.1473 - mse: 8.1473 - val_loss: 8.4891 - val_mse: 8.4891\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.7359 - mse: 7.7359 - val_loss: 7.7265 - val_mse: 7.7265\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.7298 - mse: 7.7298 - val_loss: 7.0421 - val_mse: 7.0421\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.2653 - mse: 6.2653 - val_loss: 6.4365 - val_mse: 6.4365\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.1988 - mse: 6.1988 - val_loss: 5.9031 - val_mse: 5.9031\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.4945 - mse: 6.4945 - val_loss: 5.4427 - val_mse: 5.4427\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.4862 - mse: 5.4862 - val_loss: 5.0454 - val_mse: 5.0454\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.5212 - mse: 4.5212 - val_loss: 4.7024 - val_mse: 4.7024\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6107 - mse: 4.6107 - val_loss: 4.4071 - val_mse: 4.4071\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.7836 - mse: 4.7836 - val_loss: 4.1571 - val_mse: 4.1571\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.7023 - mse: 4.7023 - val_loss: 3.9491 - val_mse: 3.9491\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.4957 - mse: 4.4957 - val_loss: 3.7775 - val_mse: 3.7775\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.3497 - mse: 4.3497 - val_loss: 3.6362 - val_mse: 3.6362\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.4678 - mse: 3.4678 - val_loss: 3.5187 - val_mse: 3.5187\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7495 - mse: 3.7495 - val_loss: 3.4208 - val_mse: 3.4208\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.4134 - mse: 4.4134 - val_loss: 3.3415 - val_mse: 3.3415\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.7543 - mse: 3.7543 - val_loss: 3.2777 - val_mse: 3.2777\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.5679 - mse: 3.5679 - val_loss: 3.2249 - val_mse: 3.2249\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.1343 - mse: 3.1343 - val_loss: 3.1785 - val_mse: 3.1785\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8344 - mse: 3.8344 - val_loss: 3.1375 - val_mse: 3.1375\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.1769 - mse: 3.1769 - val_loss: 3.0964 - val_mse: 3.0964\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.5189 - mse: 3.5189 - val_loss: 3.0572 - val_mse: 3.0572\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.3663 - mse: 3.3663 - val_loss: 3.0175 - val_mse: 3.0175\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.0230 - mse: 3.0230 - val_loss: 2.9760 - val_mse: 2.9760\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.3393 - mse: 3.3393 - val_loss: 2.9338 - val_mse: 2.9338\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0178 - mse: 3.0178 - val_loss: 2.8895 - val_mse: 2.8895\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6229 - mse: 2.6229 - val_loss: 2.8444 - val_mse: 2.8444\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9594 - mse: 2.9594 - val_loss: 2.7963 - val_mse: 2.7963\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6526 - mse: 2.6526 - val_loss: 2.7473 - val_mse: 2.7473\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7272 - mse: 2.7272 - val_loss: 2.6972 - val_mse: 2.6972\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 2.8466 - mse: 2.8466 - val_loss: 2.6476 - val_mse: 2.6476\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.9608 - mse: 2.9608 - val_loss: 2.5940 - val_mse: 2.5940\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.3872 - mse: 2.3872 - val_loss: 2.5373 - val_mse: 2.5373\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5619 - mse: 2.5619 - val_loss: 2.4774 - val_mse: 2.4774\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.1879 - mse: 2.1879 - val_loss: 2.4199 - val_mse: 2.4199\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0497 - mse: 2.0497 - val_loss: 2.3620 - val_mse: 2.3620\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.1592 - mse: 2.1592 - val_loss: 2.3039 - val_mse: 2.3039\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0104 - mse: 2.0104 - val_loss: 2.2513 - val_mse: 2.2513\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.0293 - mse: 2.0293 - val_loss: 2.1972 - val_mse: 2.1972\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2348 - mse: 2.2348 - val_loss: 2.1429 - val_mse: 2.1429\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5506 - mse: 1.5506 - val_loss: 2.0903 - val_mse: 2.0903\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2086 - mse: 2.2086 - val_loss: 2.0379 - val_mse: 2.0379\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4703 - mse: 1.4703 - val_loss: 1.9885 - val_mse: 1.9885\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6718 - mse: 1.6718 - val_loss: 1.9326 - val_mse: 1.9326\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4443 - mse: 1.4443 - val_loss: 1.8734 - val_mse: 1.8734\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4375 - mse: 1.4375 - val_loss: 1.8159 - val_mse: 1.8159\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4989 - mse: 1.4989 - val_loss: 1.7626 - val_mse: 1.7626\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6295 - mse: 1.6295 - val_loss: 1.7140 - val_mse: 1.7140\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3289 - mse: 1.3289 - val_loss: 1.6658 - val_mse: 1.6658\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4771 - mse: 1.4771 - val_loss: 1.6169 - val_mse: 1.6169\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.5985 - mse: 1.5985 - val_loss: 1.5681 - val_mse: 1.5681\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.5481 - mse: 1.5481 - val_loss: 1.5222 - val_mse: 1.5222\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.5374 - mse: 1.5374 - val_loss: 1.4815 - val_mse: 1.4815\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3991 - mse: 1.3991 - val_loss: 1.4440 - val_mse: 1.4440\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8945 - mse: 1.8945 - val_loss: 1.4116 - val_mse: 1.4116\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1253 - mse: 1.1253 - val_loss: 1.3806 - val_mse: 1.3806\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4327 - mse: 1.4327 - val_loss: 1.3533 - val_mse: 1.3533\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3391 - mse: 1.3391 - val_loss: 1.3279 - val_mse: 1.3279\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1378 - mse: 1.1378 - val_loss: 1.3044 - val_mse: 1.3044\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2996 - mse: 1.2996 - val_loss: 1.2880 - val_mse: 1.2880\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6432 - mse: 1.6432 - val_loss: 1.2707 - val_mse: 1.2707\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1988 - mse: 1.1988 - val_loss: 1.2525 - val_mse: 1.2525\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9135 - mse: 0.9135 - val_loss: 1.2335 - val_mse: 1.2335\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4675 - mse: 1.4675 - val_loss: 1.2174 - val_mse: 1.2174\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3009 - mse: 1.3009 - val_loss: 1.2005 - val_mse: 1.2005\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6493 - mse: 1.6493 - val_loss: 1.1912 - val_mse: 1.1912\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9670 - mse: 0.9670 - val_loss: 1.1822 - val_mse: 1.1822\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8908 - mse: 0.8908 - val_loss: 1.1738 - val_mse: 1.1738\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0794 - mse: 1.0794 - val_loss: 1.1723 - val_mse: 1.1723\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8489 - mse: 0.8489 - val_loss: 1.1708 - val_mse: 1.1708\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1530 - mse: 1.1530 - val_loss: 1.1675 - val_mse: 1.1675\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0227 - mse: 1.0227 - val_loss: 1.1623 - val_mse: 1.1623\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2780 - mse: 1.2780 - val_loss: 1.1536 - val_mse: 1.1536\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9761 - mse: 0.9761 - val_loss: 1.1490 - val_mse: 1.1490\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7323 - mse: 0.7323 - val_loss: 1.1410 - val_mse: 1.1410\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0081 - mse: 1.0081 - val_loss: 1.1370 - val_mse: 1.1370\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0578 - mse: 1.0578 - val_loss: 1.1321 - val_mse: 1.1321\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0573 - mse: 1.0573 - val_loss: 1.1373 - val_mse: 1.1373\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1057 - mse: 1.1057 - val_loss: 1.1377 - val_mse: 1.1377\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9791 - mse: 0.9791 - val_loss: 1.1406 - val_mse: 1.1406\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0399 - mse: 1.0399 - val_loss: 1.1397 - val_mse: 1.1397\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7854 - mse: 0.7854 - val_loss: 1.1301 - val_mse: 1.1301\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9127 - mse: 0.9127 - val_loss: 1.1280 - val_mse: 1.1280\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0774 - mse: 1.0774 - val_loss: 1.1235 - val_mse: 1.1235\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9773 - mse: 0.9773 - val_loss: 1.1207 - val_mse: 1.1207\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8906 - mse: 0.8906 - val_loss: 1.1110 - val_mse: 1.1110\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9490 - mse: 0.9490 - val_loss: 1.1080 - val_mse: 1.1080\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7253 - mse: 0.7253 - val_loss: 1.1077 - val_mse: 1.1077\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1767 - mse: 1.1767 - val_loss: 1.1036 - val_mse: 1.1036\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1304 - mse: 1.1304 - val_loss: 1.1046 - val_mse: 1.1046\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9423 - mse: 0.9423 - val_loss: 1.0961 - val_mse: 1.0961\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7981 - mse: 0.7981 - val_loss: 1.0855 - val_mse: 1.0855\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7292 - mse: 0.7292 - val_loss: 1.0747 - val_mse: 1.0747\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9014 - mse: 0.9014 - val_loss: 1.0707 - val_mse: 1.0707\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5989 - mse: 0.5989 - val_loss: 1.0714 - val_mse: 1.0714\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8520 - mse: 0.8520 - val_loss: 1.0707 - val_mse: 1.0707\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0004 - mse: 1.0004 - val_loss: 1.0728 - val_mse: 1.0728\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6051 - mse: 0.6051 - val_loss: 1.0789 - val_mse: 1.0789\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3512 - mse: 1.3512 - val_loss: 1.0766 - val_mse: 1.0766\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7699 - mse: 0.7699 - val_loss: 1.0704 - val_mse: 1.0704\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9855 - mse: 0.9855 - val_loss: 1.0662 - val_mse: 1.0662\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1498 - mse: 1.1498 - val_loss: 1.0623 - val_mse: 1.0623\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9081 - mse: 0.9081 - val_loss: 1.0610 - val_mse: 1.0610\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2331 - mse: 1.2331 - val_loss: 1.0563 - val_mse: 1.0563\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0330 - mse: 1.0330 - val_loss: 1.0503 - val_mse: 1.0503\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9395 - mse: 0.9395 - val_loss: 1.0472 - val_mse: 1.0472\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8725 - mse: 0.8725 - val_loss: 1.0430 - val_mse: 1.0430\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7604 - mse: 0.7604 - val_loss: 1.0376 - val_mse: 1.0376\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8560 - mse: 0.8560 - val_loss: 1.0312 - val_mse: 1.0312\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7130 - mse: 0.7130 - val_loss: 1.0241 - val_mse: 1.0241\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6592 - mse: 0.6592 - val_loss: 1.0203 - val_mse: 1.0203\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9220 - mse: 0.9220 - val_loss: 1.0166 - val_mse: 1.0166\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8202 - mse: 0.8202 - val_loss: 1.0065 - val_mse: 1.0065\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0283 - mse: 1.0283 - val_loss: 0.9954 - val_mse: 0.9954\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7425 - mse: 0.7425 - val_loss: 0.9863 - val_mse: 0.9863\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9297 - mse: 0.9297 - val_loss: 0.9756 - val_mse: 0.9756\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6428 - mse: 0.6428 - val_loss: 0.9670 - val_mse: 0.9670\n",
      "Epoch 125/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8523 - mse: 0.8523 - val_loss: 0.9604 - val_mse: 0.9604\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7338 - mse: 0.7338 - val_loss: 0.9542 - val_mse: 0.9542\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9000 - mse: 0.9000 - val_loss: 0.9508 - val_mse: 0.9508\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8681 - mse: 0.8681 - val_loss: 0.9462 - val_mse: 0.9462\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6140 - mse: 0.6140 - val_loss: 0.9296 - val_mse: 0.9296\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0792 - mse: 1.0792 - val_loss: 0.9155 - val_mse: 0.9155\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0727 - mse: 1.0727 - val_loss: 0.9053 - val_mse: 0.9053\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7998 - mse: 0.7998 - val_loss: 0.9031 - val_mse: 0.9031\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8419 - mse: 0.8419 - val_loss: 0.9031 - val_mse: 0.9031\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5783 - mse: 0.5783 - val_loss: 0.9149 - val_mse: 0.9149\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9258 - mse: 0.9258 - val_loss: 0.9181 - val_mse: 0.9181\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6286 - mse: 0.6286 - val_loss: 0.9227 - val_mse: 0.9227\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9635 - mse: 0.9635 - val_loss: 0.9347 - val_mse: 0.9347\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9131 - mse: 0.9131 - val_loss: 0.9551 - val_mse: 0.9551\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8598 - mse: 0.8598 - val_loss: 0.9673 - val_mse: 0.9673\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9827 - mse: 0.9827 - val_loss: 0.9864 - val_mse: 0.9864\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0179 - mse: 1.0179 - val_loss: 0.9914 - val_mse: 0.9914\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9416 - mse: 0.9416 - val_loss: 0.9890 - val_mse: 0.9890\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7071 - mse: 0.7071 - val_loss: 0.9862 - val_mse: 0.9862\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6218 - mse: 0.6218 - val_loss: 0.9836 - val_mse: 0.9836\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6100 - mse: 0.6100 - val_loss: 0.9833 - val_mse: 0.9833\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7969 - mse: 0.7969 - val_loss: 0.9754 - val_mse: 0.9754\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5517 - mse: 0.5517 - val_loss: 0.9660 - val_mse: 0.9660\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9030 - mse: 0.9030 - val_loss: 0.9450 - val_mse: 0.9450\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8962 - mse: 0.8962 - val_loss: 0.9282 - val_mse: 0.9282\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7972 - mse: 0.7972 - val_loss: 0.9097 - val_mse: 0.9097\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9833 - mse: 0.9833 - val_loss: 0.8922 - val_mse: 0.8922\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7271 - mse: 0.7271 - val_loss: 0.8856 - val_mse: 0.8856\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8639 - mse: 0.8639 - val_loss: 0.8807 - val_mse: 0.8807\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6933 - mse: 0.6933 - val_loss: 0.8805 - val_mse: 0.8805\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5666 - mse: 0.5666 - val_loss: 0.8730 - val_mse: 0.8730\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8274 - mse: 0.8274 - val_loss: 0.8697 - val_mse: 0.8697\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0476 - mse: 1.0476 - val_loss: 0.8697 - val_mse: 0.8697\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5521 - mse: 0.5521 - val_loss: 0.8716 - val_mse: 0.8716\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8257 - mse: 0.8257 - val_loss: 0.8867 - val_mse: 0.8867\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7333 - mse: 0.7333 - val_loss: 0.9051 - val_mse: 0.9051\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7479 - mse: 0.7479 - val_loss: 0.9307 - val_mse: 0.9307\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7589 - mse: 0.7589 - val_loss: 0.9569 - val_mse: 0.9569\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8993 - mse: 0.8993 - val_loss: 0.9757 - val_mse: 0.9757\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5510 - mse: 0.5510 - val_loss: 0.9961 - val_mse: 0.9961\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8855 - mse: 0.8855 - val_loss: 1.0097 - val_mse: 1.0097\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5663 - mse: 0.5663 - val_loss: 1.0232 - val_mse: 1.0232\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7210 - mse: 0.7210 - val_loss: 1.0315 - val_mse: 1.0315\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6655 - mse: 0.6655 - val_loss: 1.0279 - val_mse: 1.0279\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7250 - mse: 0.7250 - val_loss: 1.0193 - val_mse: 1.0193\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.8453 - mse: 0.8453 - val_loss: 1.0044 - val_mse: 1.0044\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7500 - mse: 0.7500 - val_loss: 1.0018 - val_mse: 1.0018\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8906 - mse: 0.8906 - val_loss: 0.9937 - val_mse: 0.9937\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7195 - mse: 0.7195 - val_loss: 0.9827 - val_mse: 0.9827\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6264 - mse: 0.6264 - val_loss: 0.9743 - val_mse: 0.9743\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9047 - mse: 0.9047 - val_loss: 0.9593 - val_mse: 0.9593\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5515 - mse: 0.5515 - val_loss: 0.9493 - val_mse: 0.9493\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5951 - mse: 0.5951 - val_loss: 0.9397 - val_mse: 0.9397\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "dnn_model = tuner.hypermodel.build(best_hps)\n",
    "#best_model = tuner.get_best_models()[0]\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_epochs=500\n",
    "#history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=32, validation_split=0.2,callbacks=callbacks_list)\n",
    "#history = model.fit(X_train, y_train, epochs=n_epochs, bbatch_size=32, validation_split=0.2,callbacks=callbacks_list)\n",
    "history = dnn_model.fit(X_train, y_train, epochs=n_epochs, batch_size=32, validation_data=(X_test,y_test),callbacks=[keras.callbacks.EarlyStopping(patience=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee283f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9397 - mse: 0.9397\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mse = dnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2c7b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('DNN Model loss Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3516f9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.9607344 , 5.8471756 ],\n",
       "       [5.9948945 , 3.0514681 ],\n",
       "       [2.0353444 , 3.3720703 ],\n",
       "       [2.0400815 , 5.574958  ],\n",
       "       [4.071133  , 1.0823162 ],\n",
       "       [3.5559497 , 5.3480725 ],\n",
       "       [2.2066975 , 5.231998  ],\n",
       "       [4.2563114 , 5.6950192 ],\n",
       "       [3.889746  , 3.2348247 ],\n",
       "       [2.5824337 , 6.709407  ],\n",
       "       [2.3043323 , 1.5595971 ],\n",
       "       [3.0631642 , 0.99782115]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=dnn_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2633c699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on new data in m: 0.94\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.97\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 25.68\n",
      "R2 score is in percent: 77.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, y_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(y_test,y_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1919115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 6, 'n_estimators': 200}\n",
      "Mean Squared Error in meter: 0.166\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.407\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 12.935\n",
      "R2 score is in percent: 94.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(y_pred, y_test)\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(y_pred)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,RF_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, RF_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f470c9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 0.77\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.88\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 29.38\n",
      "R2 score is in percent: 75.25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(y_pred, y_test)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(y_pred)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "rmse=sqrt(mean_squared_error(y_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(y_test,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "149444d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "dnn_model.save('my_DNN_Regrr_model_xy_no_feature_extraction.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1399150d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x1b2938f8910>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = load_model('my_DNN_Regrr_model_xy_no_feature_extraction.h5')\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2f534fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff=pd.read_csv('D:/testt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d82b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff.drop(['AP1RSS','AP2RSS','AP3RSS'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7be3d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>A_P_1_R_T_T</th>\n",
       "      <th>A_P_1_S_T_D_E_V</th>\n",
       "      <th>A_P_2_R_T_T</th>\n",
       "      <th>A_P_2_S_T_D_E_V</th>\n",
       "      <th>A_P_3_R_T_T</th>\n",
       "      <th>A_P_3_S_T_D_E_V</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7.707727</td>\n",
       "      <td>1.848299</td>\n",
       "      <td>1.044589</td>\n",
       "      <td>0.141585</td>\n",
       "      <td>10.718671</td>\n",
       "      <td>1.587250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.365578</td>\n",
       "      <td>2.194745</td>\n",
       "      <td>2.080257</td>\n",
       "      <td>1.555030</td>\n",
       "      <td>9.977772</td>\n",
       "      <td>0.945368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5.488693</td>\n",
       "      <td>1.054929</td>\n",
       "      <td>2.950386</td>\n",
       "      <td>0.304836</td>\n",
       "      <td>8.557814</td>\n",
       "      <td>0.499264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.132376</td>\n",
       "      <td>0.544648</td>\n",
       "      <td>3.034266</td>\n",
       "      <td>0.205060</td>\n",
       "      <td>10.871507</td>\n",
       "      <td>0.390431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3.839836</td>\n",
       "      <td>0.735971</td>\n",
       "      <td>4.407919</td>\n",
       "      <td>0.549324</td>\n",
       "      <td>7.114536</td>\n",
       "      <td>1.709690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.905374</td>\n",
       "      <td>0.370085</td>\n",
       "      <td>7.411877</td>\n",
       "      <td>0.148853</td>\n",
       "      <td>6.012209</td>\n",
       "      <td>0.291436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2.661546</td>\n",
       "      <td>2.264162</td>\n",
       "      <td>6.661156</td>\n",
       "      <td>1.620737</td>\n",
       "      <td>7.171201</td>\n",
       "      <td>0.941743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1.216686</td>\n",
       "      <td>0.418363</td>\n",
       "      <td>10.075713</td>\n",
       "      <td>0.189108</td>\n",
       "      <td>5.769130</td>\n",
       "      <td>1.008381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7.930118</td>\n",
       "      <td>0.178429</td>\n",
       "      <td>2.441135</td>\n",
       "      <td>0.839498</td>\n",
       "      <td>11.735521</td>\n",
       "      <td>0.445599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6.574464</td>\n",
       "      <td>1.783223</td>\n",
       "      <td>3.448252</td>\n",
       "      <td>0.176038</td>\n",
       "      <td>9.213649</td>\n",
       "      <td>1.413693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5.996126</td>\n",
       "      <td>0.365672</td>\n",
       "      <td>3.714013</td>\n",
       "      <td>0.125938</td>\n",
       "      <td>7.652355</td>\n",
       "      <td>0.265994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4.413265</td>\n",
       "      <td>0.669002</td>\n",
       "      <td>5.416025</td>\n",
       "      <td>0.759153</td>\n",
       "      <td>7.635213</td>\n",
       "      <td>0.932911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4.296948</td>\n",
       "      <td>0.446524</td>\n",
       "      <td>5.050100</td>\n",
       "      <td>1.500059</td>\n",
       "      <td>6.843512</td>\n",
       "      <td>1.083697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3.724204</td>\n",
       "      <td>0.732465</td>\n",
       "      <td>5.832049</td>\n",
       "      <td>0.547188</td>\n",
       "      <td>4.436995</td>\n",
       "      <td>0.746397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4.065966</td>\n",
       "      <td>0.748815</td>\n",
       "      <td>6.722876</td>\n",
       "      <td>0.480757</td>\n",
       "      <td>5.809646</td>\n",
       "      <td>0.625828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4.317167</td>\n",
       "      <td>0.357696</td>\n",
       "      <td>8.827701</td>\n",
       "      <td>0.213574</td>\n",
       "      <td>4.550802</td>\n",
       "      <td>1.061153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.198304</td>\n",
       "      <td>1.290408</td>\n",
       "      <td>3.573971</td>\n",
       "      <td>1.100223</td>\n",
       "      <td>9.933865</td>\n",
       "      <td>0.752753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7.266628</td>\n",
       "      <td>1.173948</td>\n",
       "      <td>3.682856</td>\n",
       "      <td>0.312896</td>\n",
       "      <td>8.486539</td>\n",
       "      <td>0.403170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.336682</td>\n",
       "      <td>0.411186</td>\n",
       "      <td>4.087176</td>\n",
       "      <td>0.440170</td>\n",
       "      <td>7.797669</td>\n",
       "      <td>0.198211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5.022169</td>\n",
       "      <td>0.867000</td>\n",
       "      <td>4.949401</td>\n",
       "      <td>0.951073</td>\n",
       "      <td>6.744881</td>\n",
       "      <td>2.050512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3.673121</td>\n",
       "      <td>0.806182</td>\n",
       "      <td>8.724427</td>\n",
       "      <td>1.660813</td>\n",
       "      <td>4.563089</td>\n",
       "      <td>0.355049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9.147432</td>\n",
       "      <td>0.462575</td>\n",
       "      <td>5.699894</td>\n",
       "      <td>1.928789</td>\n",
       "      <td>10.939863</td>\n",
       "      <td>1.732795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.581237</td>\n",
       "      <td>0.954260</td>\n",
       "      <td>4.836910</td>\n",
       "      <td>0.741301</td>\n",
       "      <td>7.922546</td>\n",
       "      <td>0.491107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6.957905</td>\n",
       "      <td>1.404438</td>\n",
       "      <td>5.006211</td>\n",
       "      <td>2.035923</td>\n",
       "      <td>9.491720</td>\n",
       "      <td>0.237863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7.315369</td>\n",
       "      <td>2.006884</td>\n",
       "      <td>7.907026</td>\n",
       "      <td>0.591230</td>\n",
       "      <td>7.464645</td>\n",
       "      <td>0.278662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.336127</td>\n",
       "      <td>0.758420</td>\n",
       "      <td>6.714174</td>\n",
       "      <td>0.199994</td>\n",
       "      <td>5.165166</td>\n",
       "      <td>1.024754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.770708</td>\n",
       "      <td>1.174054</td>\n",
       "      <td>6.913629</td>\n",
       "      <td>0.316632</td>\n",
       "      <td>3.556297</td>\n",
       "      <td>0.822694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>6.580740</td>\n",
       "      <td>0.309046</td>\n",
       "      <td>7.790735</td>\n",
       "      <td>0.281008</td>\n",
       "      <td>3.348171</td>\n",
       "      <td>0.336740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>7.145828</td>\n",
       "      <td>1.374866</td>\n",
       "      <td>11.473872</td>\n",
       "      <td>0.299344</td>\n",
       "      <td>3.884900</td>\n",
       "      <td>0.649381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>11.466279</td>\n",
       "      <td>0.809003</td>\n",
       "      <td>5.983782</td>\n",
       "      <td>0.990327</td>\n",
       "      <td>8.905374</td>\n",
       "      <td>0.883837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9.288792</td>\n",
       "      <td>0.752261</td>\n",
       "      <td>5.557330</td>\n",
       "      <td>0.211597</td>\n",
       "      <td>9.406587</td>\n",
       "      <td>0.919855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9.414815</td>\n",
       "      <td>0.524892</td>\n",
       "      <td>5.769318</td>\n",
       "      <td>1.316199</td>\n",
       "      <td>7.227747</td>\n",
       "      <td>1.820406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>6.484015</td>\n",
       "      <td>0.611780</td>\n",
       "      <td>7.812446</td>\n",
       "      <td>0.707883</td>\n",
       "      <td>2.728191</td>\n",
       "      <td>0.206484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>8.353455</td>\n",
       "      <td>0.480833</td>\n",
       "      <td>9.475557</td>\n",
       "      <td>0.141216</td>\n",
       "      <td>1.861306</td>\n",
       "      <td>1.372664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>9.995851</td>\n",
       "      <td>1.125597</td>\n",
       "      <td>5.947130</td>\n",
       "      <td>0.159604</td>\n",
       "      <td>9.403380</td>\n",
       "      <td>0.123617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>10.438876</td>\n",
       "      <td>0.394583</td>\n",
       "      <td>7.118710</td>\n",
       "      <td>0.445696</td>\n",
       "      <td>9.829392</td>\n",
       "      <td>0.493212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9.264814</td>\n",
       "      <td>0.273084</td>\n",
       "      <td>8.814989</td>\n",
       "      <td>1.123449</td>\n",
       "      <td>7.161158</td>\n",
       "      <td>0.400470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5.759847</td>\n",
       "      <td>0.884673</td>\n",
       "      <td>9.734726</td>\n",
       "      <td>0.835044</td>\n",
       "      <td>1.902758</td>\n",
       "      <td>0.174248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7.157109</td>\n",
       "      <td>0.534206</td>\n",
       "      <td>11.125667</td>\n",
       "      <td>0.375055</td>\n",
       "      <td>0.478882</td>\n",
       "      <td>0.980824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x  y  A_P_1_R_T_T  A_P_1_S_T_D_E_V  A_P_2_R_T_T  A_P_2_S_T_D_E_V  \\\n",
       "0   1  1     7.707727         1.848299     1.044589         0.141585   \n",
       "1   1  2     7.365578         2.194745     2.080257         1.555030   \n",
       "2   1  3     5.488693         1.054929     2.950386         0.304836   \n",
       "3   1  4     5.132376         0.544648     3.034266         0.205060   \n",
       "4   1  5     3.839836         0.735971     4.407919         0.549324   \n",
       "5   1  6     4.905374         0.370085     7.411877         0.148853   \n",
       "6   1  7     2.661546         2.264162     6.661156         1.620737   \n",
       "7   1  8     1.216686         0.418363    10.075713         0.189108   \n",
       "8   2  1     7.930118         0.178429     2.441135         0.839498   \n",
       "9   2  2     6.574464         1.783223     3.448252         0.176038   \n",
       "10  2  3     5.996126         0.365672     3.714013         0.125938   \n",
       "11  2  4     4.413265         0.669002     5.416025         0.759153   \n",
       "12  2  5     4.296948         0.446524     5.050100         1.500059   \n",
       "13  2  6     3.724204         0.732465     5.832049         0.547188   \n",
       "14  2  7     4.065966         0.748815     6.722876         0.480757   \n",
       "15  2  8     4.317167         0.357696     8.827701         0.213574   \n",
       "16  3  1     8.198304         1.290408     3.573971         1.100223   \n",
       "17  3  2     7.266628         1.173948     3.682856         0.312896   \n",
       "18  3  3     7.336682         0.411186     4.087176         0.440170   \n",
       "19  3  4     5.022169         0.867000     4.949401         0.951073   \n",
       "20  3  8     3.673121         0.806182     8.724427         1.660813   \n",
       "21  4  1     9.147432         0.462575     5.699894         1.928789   \n",
       "22  4  2     7.581237         0.954260     4.836910         0.741301   \n",
       "23  4  3     6.957905         1.404438     5.006211         2.035923   \n",
       "24  4  4     7.315369         2.006884     7.907026         0.591230   \n",
       "25  4  5     5.336127         0.758420     6.714174         0.199994   \n",
       "26  4  6     5.770708         1.174054     6.913629         0.316632   \n",
       "27  4  7     6.580740         0.309046     7.790735         0.281008   \n",
       "28  4  8     7.145828         1.374866    11.473872         0.299344   \n",
       "29  5  1    11.466279         0.809003     5.983782         0.990327   \n",
       "30  5  2     9.288792         0.752261     5.557330         0.211597   \n",
       "31  5  3     9.414815         0.524892     5.769318         1.316199   \n",
       "32  5  7     6.484015         0.611780     7.812446         0.707883   \n",
       "33  5  8     8.353455         0.480833     9.475557         0.141216   \n",
       "34  6  1     9.995851         1.125597     5.947130         0.159604   \n",
       "35  6  2    10.438876         0.394583     7.118710         0.445696   \n",
       "36  6  3     9.264814         0.273084     8.814989         1.123449   \n",
       "37  6  7     5.759847         0.884673     9.734726         0.835044   \n",
       "38  6  8     7.157109         0.534206    11.125667         0.375055   \n",
       "\n",
       "    A_P_3_R_T_T  A_P_3_S_T_D_E_V  \n",
       "0     10.718671         1.587250  \n",
       "1      9.977772         0.945368  \n",
       "2      8.557814         0.499264  \n",
       "3     10.871507         0.390431  \n",
       "4      7.114536         1.709690  \n",
       "5      6.012209         0.291436  \n",
       "6      7.171201         0.941743  \n",
       "7      5.769130         1.008381  \n",
       "8     11.735521         0.445599  \n",
       "9      9.213649         1.413693  \n",
       "10     7.652355         0.265994  \n",
       "11     7.635213         0.932911  \n",
       "12     6.843512         1.083697  \n",
       "13     4.436995         0.746397  \n",
       "14     5.809646         0.625828  \n",
       "15     4.550802         1.061153  \n",
       "16     9.933865         0.752753  \n",
       "17     8.486539         0.403170  \n",
       "18     7.797669         0.198211  \n",
       "19     6.744881         2.050512  \n",
       "20     4.563089         0.355049  \n",
       "21    10.939863         1.732795  \n",
       "22     7.922546         0.491107  \n",
       "23     9.491720         0.237863  \n",
       "24     7.464645         0.278662  \n",
       "25     5.165166         1.024754  \n",
       "26     3.556297         0.822694  \n",
       "27     3.348171         0.336740  \n",
       "28     3.884900         0.649381  \n",
       "29     8.905374         0.883837  \n",
       "30     9.406587         0.919855  \n",
       "31     7.227747         1.820406  \n",
       "32     2.728191         0.206484  \n",
       "33     1.861306         1.372664  \n",
       "34     9.403380         0.123617  \n",
       "35     9.829392         0.493212  \n",
       "36     7.161158         0.400470  \n",
       "37     1.902758         0.174248  \n",
       "38     0.478882         0.980824  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped = dfff.groupby(['x', 'y']).mean()\n",
    "#grouped = pd.concat([grouped, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "df = grouped.reset_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e055356",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=df.iloc[:,2:] \n",
    "output_data = df.iloc[:, :2]\n",
    "input_data=np.array(input_data.values)\n",
    "output_data=np.array(output_data.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be28078a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 2)\n",
      "(39, 6)\n"
     ]
    }
   ],
   "source": [
    "XX=input_data\n",
    "yy=output_data\n",
    "print(output_data.shape)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dafd5015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "XX= sc.fit_transform(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e5fa639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.5370231 , 0.5251437 ],\n",
       "       [1.6725075 , 1.2140747 ],\n",
       "       [1.2262003 , 2.976258  ],\n",
       "       [0.8684129 , 3.1934068 ],\n",
       "       [2.052509  , 3.9713476 ],\n",
       "       [1.9156508 , 5.8893585 ],\n",
       "       [2.1683965 , 5.4972725 ],\n",
       "       [1.4579837 , 7.976592  ],\n",
       "       [3.054468  , 1.3394442 ],\n",
       "       [1.7424837 , 1.7518016 ],\n",
       "       [1.9056206 , 3.4102674 ],\n",
       "       [1.744349  , 4.4972124 ],\n",
       "       [2.3709664 , 4.802902  ],\n",
       "       [2.1622295 , 5.939438  ],\n",
       "       [1.8819158 , 5.839184  ],\n",
       "       [2.6653032 , 6.8879633 ],\n",
       "       [2.8408356 , 1.2009497 ],\n",
       "       [2.4223301 , 2.0385182 ],\n",
       "       [3.1987367 , 2.6249921 ],\n",
       "       [2.7378557 , 3.757342  ],\n",
       "       [3.3608837 , 7.6810937 ],\n",
       "       [4.093892  , 1.6090379 ],\n",
       "       [3.2945251 , 2.3480458 ],\n",
       "       [3.6176667 , 2.9290755 ],\n",
       "       [3.8843586 , 3.877673  ],\n",
       "       [2.823187  , 5.2322264 ],\n",
       "       [3.7343433 , 5.723753  ],\n",
       "       [4.3735566 , 5.9441833 ],\n",
       "       [5.109478  , 7.296829  ],\n",
       "       [5.7478786 , 0.88887817],\n",
       "       [4.233562  , 1.5916129 ],\n",
       "       [4.6792307 , 2.478793  ],\n",
       "       [4.862017  , 6.2979403 ],\n",
       "       [5.891993  , 6.0612273 ],\n",
       "       [5.0678864 , 1.0856566 ],\n",
       "       [5.613836  , 1.4348896 ],\n",
       "       [5.867772  , 3.4489844 ],\n",
       "       [5.087215  , 7.7338424 ],\n",
       "       [5.849526  , 7.849223  ]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predd=loaded_model.predict(XX)\n",
    "y_predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f540aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on new data in mm: 0.39\n",
      "Root Mean Squared Error (RMSE) on new data in mm: 0.63\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 19.11\n",
      "R2 score is in percent: 90.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(yy, y_predd)\n",
    "print('Mean Squared Error (MSE) on new data in mm: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy, y_predd)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in mm: {:.2f}'.format(rmse))\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,y_predd)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, y_predd)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "044d2de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.537023</td>\n",
       "      <td>0.525144</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.672508</td>\n",
       "      <td>1.214075</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.226200</td>\n",
       "      <td>2.976258</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.868413</td>\n",
       "      <td>3.193407</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.052509</td>\n",
       "      <td>3.971348</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.915651</td>\n",
       "      <td>5.889359</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.168396</td>\n",
       "      <td>5.497272</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.457984</td>\n",
       "      <td>7.976592</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.054468</td>\n",
       "      <td>1.339444</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.742484</td>\n",
       "      <td>1.751802</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.905621</td>\n",
       "      <td>3.410267</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.744349</td>\n",
       "      <td>4.497212</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.370966</td>\n",
       "      <td>4.802902</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.162230</td>\n",
       "      <td>5.939438</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.881916</td>\n",
       "      <td>5.839184</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.665303</td>\n",
       "      <td>6.887963</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.840836</td>\n",
       "      <td>1.200950</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.422330</td>\n",
       "      <td>2.038518</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.198737</td>\n",
       "      <td>2.624992</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.737856</td>\n",
       "      <td>3.757342</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.360884</td>\n",
       "      <td>7.681094</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.093892</td>\n",
       "      <td>1.609038</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.294525</td>\n",
       "      <td>2.348046</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.617667</td>\n",
       "      <td>2.929075</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3.884359</td>\n",
       "      <td>3.877673</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.823187</td>\n",
       "      <td>5.232226</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.734343</td>\n",
       "      <td>5.723753</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.373557</td>\n",
       "      <td>5.944183</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.109478</td>\n",
       "      <td>7.296829</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.747879</td>\n",
       "      <td>0.888878</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.233562</td>\n",
       "      <td>1.591613</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.679231</td>\n",
       "      <td>2.478793</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.862017</td>\n",
       "      <td>6.297940</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.891993</td>\n",
       "      <td>6.061227</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.067886</td>\n",
       "      <td>1.085657</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.613836</td>\n",
       "      <td>1.434890</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.867772</td>\n",
       "      <td>3.448984</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.087215</td>\n",
       "      <td>7.733842</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.849526</td>\n",
       "      <td>7.849223</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_x  predicted_y  x  y\n",
       "0      1.537023     0.525144  1  1\n",
       "1      1.672508     1.214075  1  2\n",
       "2      1.226200     2.976258  1  3\n",
       "3      0.868413     3.193407  1  4\n",
       "4      2.052509     3.971348  1  5\n",
       "5      1.915651     5.889359  1  6\n",
       "6      2.168396     5.497272  1  7\n",
       "7      1.457984     7.976592  1  8\n",
       "8      3.054468     1.339444  2  1\n",
       "9      1.742484     1.751802  2  2\n",
       "10     1.905621     3.410267  2  3\n",
       "11     1.744349     4.497212  2  4\n",
       "12     2.370966     4.802902  2  5\n",
       "13     2.162230     5.939438  2  6\n",
       "14     1.881916     5.839184  2  7\n",
       "15     2.665303     6.887963  2  8\n",
       "16     2.840836     1.200950  3  1\n",
       "17     2.422330     2.038518  3  2\n",
       "18     3.198737     2.624992  3  3\n",
       "19     2.737856     3.757342  3  4\n",
       "20     3.360884     7.681094  3  8\n",
       "21     4.093892     1.609038  4  1\n",
       "22     3.294525     2.348046  4  2\n",
       "23     3.617667     2.929075  4  3\n",
       "24     3.884359     3.877673  4  4\n",
       "25     2.823187     5.232226  4  5\n",
       "26     3.734343     5.723753  4  6\n",
       "27     4.373557     5.944183  4  7\n",
       "28     5.109478     7.296829  4  8\n",
       "29     5.747879     0.888878  5  1\n",
       "30     4.233562     1.591613  5  2\n",
       "31     4.679231     2.478793  5  3\n",
       "32     4.862017     6.297940  5  7\n",
       "33     5.891993     6.061227  5  8\n",
       "34     5.067886     1.085657  6  1\n",
       "35     5.613836     1.434890  6  2\n",
       "36     5.867772     3.448984  6  3\n",
       "37     5.087215     7.733842  6  7\n",
       "38     5.849526     7.849223  6  8"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy=pd.DataFrame(yy,columns=['x','y'])\n",
    "y_predd=pd.DataFrame(y_predd, columns=['predicted_x','predicted_y'])\n",
    "df_finall_dnn = pd.DataFrame()\n",
    "df_finall_dnn = pd.concat([y_predd, yy], axis=1)\n",
    "df_finall_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eae96f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_finall_dnn.to_csv('output_data_dnn_no_ex.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5282d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 6, 'n_estimators': 200}\n",
      "Mean Squared Error in meter: 0.088\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.296\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 10.42\n",
      "R2 score is in percent: 97.64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(y_predd,yy)\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(y_predd)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(yy, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,RF_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, RF_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5ca302d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.185000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.405000</td>\n",
       "      <td>1.805000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.163333</td>\n",
       "      <td>3.198333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.220000</td>\n",
       "      <td>3.598333</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.452500</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.360000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.325833</td>\n",
       "      <td>6.685000</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.280000</td>\n",
       "      <td>7.740000</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.330000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.897500</td>\n",
       "      <td>1.986500</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.895000</td>\n",
       "      <td>3.241000</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.872500</td>\n",
       "      <td>4.467000</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.902500</td>\n",
       "      <td>4.985000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.705000</td>\n",
       "      <td>6.220000</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.758333</td>\n",
       "      <td>6.685000</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.380000</td>\n",
       "      <td>7.510000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.610000</td>\n",
       "      <td>1.230000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.777500</td>\n",
       "      <td>1.979000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.165000</td>\n",
       "      <td>2.860000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.737500</td>\n",
       "      <td>3.856250</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3.505000</td>\n",
       "      <td>7.550000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.382500</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.647500</td>\n",
       "      <td>2.150000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.035000</td>\n",
       "      <td>3.042500</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.065000</td>\n",
       "      <td>3.982500</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.270000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.845000</td>\n",
       "      <td>6.272500</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.235000</td>\n",
       "      <td>6.995000</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.555000</td>\n",
       "      <td>7.690000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.235000</td>\n",
       "      <td>1.220000</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.735000</td>\n",
       "      <td>1.765000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.992500</td>\n",
       "      <td>2.626250</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.842500</td>\n",
       "      <td>7.190833</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4.935000</td>\n",
       "      <td>7.690833</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.732500</td>\n",
       "      <td>1.225000</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.707500</td>\n",
       "      <td>1.790000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.637500</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.465000</td>\n",
       "      <td>7.275000</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.745000</td>\n",
       "      <td>7.845000</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_x  predicted_y  x  y\n",
       "0      1.185000     1.290000  1  1\n",
       "1      1.405000     1.805000  1  2\n",
       "2      1.163333     3.198333  1  3\n",
       "3      1.220000     3.598333  1  4\n",
       "4      1.452500     4.410000  1  5\n",
       "5      1.360000     6.210000  1  6\n",
       "6      1.325833     6.685000  1  7\n",
       "7      1.280000     7.740000  1  8\n",
       "8      2.330000     1.160000  2  1\n",
       "9      1.897500     1.986500  2  2\n",
       "10     1.895000     3.241000  2  3\n",
       "11     1.872500     4.467000  2  4\n",
       "12     1.902500     4.985000  2  5\n",
       "13     1.705000     6.220000  2  6\n",
       "14     1.758333     6.685000  2  7\n",
       "15     2.380000     7.510000  2  8\n",
       "16     2.610000     1.230000  3  1\n",
       "17     2.777500     1.979000  3  2\n",
       "18     3.165000     2.860000  3  3\n",
       "19     2.737500     3.856250  3  4\n",
       "20     3.505000     7.550000  3  8\n",
       "21     4.382500     1.435000  4  1\n",
       "22     3.647500     2.150000  4  2\n",
       "23     4.035000     3.042500  4  3\n",
       "24     4.065000     3.982500  4  4\n",
       "25     3.270000     5.500000  4  5\n",
       "26     3.845000     6.272500  4  6\n",
       "27     4.235000     6.995000  4  7\n",
       "28     4.555000     7.690000  4  8\n",
       "29     5.235000     1.220000  5  1\n",
       "30     4.735000     1.765000  5  2\n",
       "31     4.992500     2.626250  5  3\n",
       "32     4.842500     7.190833  5  7\n",
       "33     4.935000     7.690833  5  8\n",
       "34     5.732500     1.225000  6  1\n",
       "35     5.707500     1.790000  6  2\n",
       "36     5.637500     3.050000  6  3\n",
       "37     5.465000     7.275000  6  7\n",
       "38     5.745000     7.845000  6  8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy=pd.DataFrame(yy,columns=['x','y'])\n",
    "y_predd=pd.DataFrame(RF_pred, columns=['predicted_x','predicted_y'])\n",
    "df_finall = pd.DataFrame()\n",
    "df_finall = pd.concat([y_predd, yy], axis=1)\n",
    "df_finall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a38065ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_finall.to_csv('output_data_no_ex.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "869ac9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 0.17\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.41\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 15.72\n",
      "R2 score is in percent: 95.50\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(y_predd, yy)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(y_predd)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(yy, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "rmse=sqrt(mean_squared_error(yy, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4acfdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2e1eb4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19c3e1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 6, 'n_estimators': 200}\n",
      "Mean Squared Error in meter: 1.050\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 1.025\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 32.55\n",
      "R2 score is in percent: 72.10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(XX_train, yy_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(XX_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(yy_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy_test,RF_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy_test, RF_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fac0bcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 1.13\n",
      "Root Mean Squared Error (RMSE) on new data in m: 1.06\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 38.37\n",
      "R2 score is in percent: 70.99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from math import sqrt\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(XX_train,yy_train)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(XX_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(yy_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy_test,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy_test, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b464e56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adac2ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
