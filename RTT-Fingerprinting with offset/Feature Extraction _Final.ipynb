{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb67633a",
   "metadata": {},
   "source": [
    "**1. Importing necessary libraries and combining no. of CSV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "839db798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#combining no of csv file into one file with name combine_csv\n",
    "os.chdir('D:\\Final')\n",
    "extension='csv'\n",
    "\n",
    "all_filenames=[i for i in glob.glob('*.{}'.format(extension))]\n",
    "\n",
    "combined_csv=pd.concat([pd.read_csv(f) for f in all_filenames])\n",
    "df=combined_csv\n",
    "\n",
    "df.drop(['AP1RSS','AP2RSS','AP3RSS'],axis=1,inplace=True)\n",
    "#dff=df.assign(Product_RTT=df['AP1RTT']*df['AP2RTT']*df['AP3RTT'],Product_RTT12=df['AP1RTT']*df['AP2RTT'],Product_RTT23=df['AP2RTT']*df['AP3RTT'],Product_RTT13=df['AP1RTT']*df['AP3RTT'],square_RTT1=df['AP1RTT']*df['AP1RTT'],square_RTT2=df['AP2RTT']*df['AP2RTT'],square_RTT3=df['AP3RTT']*df['AP3RTT'])\n",
    "dff=df.assign(Product_RTT=df['AP1RTT']*df['AP2RTT']*df['AP3RTT'],Product_RTT12=df['AP1RTT']*df['AP2RTT'],Product_RTT23=df['AP2RTT']*df['AP3RTT'],Product_RTT13=df['AP1RTT']*df['AP3RTT'])\n",
    "# Group the DataFrame by 'x' and 'y'\n",
    "groups = dff.groupby(['x', 'y'])\n",
    "\n",
    "# Split the groups into two separate dataframes\n",
    "df1 = pd.concat([group.iloc[:len(group) // 2] for _, group in groups])\n",
    "df2 = pd.concat([group.iloc[len(group) // 2:] for _, group in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b840cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP1RTTA</th>\n",
       "      <th>AP1STDEVA</th>\n",
       "      <th>AP2RTTA</th>\n",
       "      <th>AP2STDEVA</th>\n",
       "      <th>AP3RTTA</th>\n",
       "      <th>AP3STDEVA</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>Product_RTTA</th>\n",
       "      <th>Product_RTT12A</th>\n",
       "      <th>Product_RTT23</th>\n",
       "      <th>Product_RTT13A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.789</td>\n",
       "      <td>0.371</td>\n",
       "      <td>1.311</td>\n",
       "      <td>0.082</td>\n",
       "      <td>11.037</td>\n",
       "      <td>1.192</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>112.702990</td>\n",
       "      <td>10.211379</td>\n",
       "      <td>14.469507</td>\n",
       "      <td>85.967193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.731</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.186</td>\n",
       "      <td>10.452</td>\n",
       "      <td>0.256</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>80.723608</td>\n",
       "      <td>7.723269</td>\n",
       "      <td>10.441548</td>\n",
       "      <td>80.804412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.731</td>\n",
       "      <td>0.442</td>\n",
       "      <td>1.155</td>\n",
       "      <td>0.171</td>\n",
       "      <td>9.368</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.649729</td>\n",
       "      <td>8.929305</td>\n",
       "      <td>10.820040</td>\n",
       "      <td>72.424008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.672</td>\n",
       "      <td>0.698</td>\n",
       "      <td>1.194</td>\n",
       "      <td>0.206</td>\n",
       "      <td>9.709</td>\n",
       "      <td>1.533</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>88.938013</td>\n",
       "      <td>9.160368</td>\n",
       "      <td>11.592546</td>\n",
       "      <td>74.487448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.643</td>\n",
       "      <td>2.609</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.151</td>\n",
       "      <td>9.748</td>\n",
       "      <td>2.350</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>77.335115</td>\n",
       "      <td>7.933434</td>\n",
       "      <td>10.118424</td>\n",
       "      <td>74.503964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>7.516</td>\n",
       "      <td>0.348</td>\n",
       "      <td>10.881</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.989</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>40.154764</td>\n",
       "      <td>81.781596</td>\n",
       "      <td>5.342571</td>\n",
       "      <td>3.690356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>7.320</td>\n",
       "      <td>0.427</td>\n",
       "      <td>10.959</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.452</td>\n",
       "      <td>1.007</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>36.259386</td>\n",
       "      <td>80.219880</td>\n",
       "      <td>4.953468</td>\n",
       "      <td>3.308640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>7.320</td>\n",
       "      <td>0.403</td>\n",
       "      <td>10.920</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.941</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>42.365232</td>\n",
       "      <td>79.934400</td>\n",
       "      <td>5.787600</td>\n",
       "      <td>3.879600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>7.320</td>\n",
       "      <td>0.302</td>\n",
       "      <td>11.194</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.991</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>36.954976</td>\n",
       "      <td>81.940080</td>\n",
       "      <td>5.048494</td>\n",
       "      <td>3.301320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>7.242</td>\n",
       "      <td>0.433</td>\n",
       "      <td>11.428</td>\n",
       "      <td>0.194</td>\n",
       "      <td>0.491</td>\n",
       "      <td>1.006</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>40.635934</td>\n",
       "      <td>82.761576</td>\n",
       "      <td>5.611148</td>\n",
       "      <td>3.555822</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14005 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AP1RTTA  AP1STDEVA  AP2RTTA  AP2STDEVA  AP3RTTA  AP3STDEVA  x  y  \\\n",
       "0      7.789      0.371    1.311      0.082   11.037      1.192  1  1   \n",
       "1      7.731      0.277    0.999      0.186   10.452      0.256  1  1   \n",
       "2      7.731      0.442    1.155      0.171    9.368      0.661  1  1   \n",
       "3      7.672      0.698    1.194      0.206    9.709      1.533  1  1   \n",
       "4      7.643      2.609    1.038      0.151    9.748      2.350  1  1   \n",
       "..       ...        ...      ...        ...      ...        ... .. ..   \n",
       "390    7.516      0.348   10.881      0.452    0.491      0.989  6  8   \n",
       "391    7.320      0.427   10.959      0.427    0.452      1.007  6  8   \n",
       "392    7.320      0.403   10.920      0.438    0.530      0.941  6  8   \n",
       "393    7.320      0.302   11.194      0.284    0.451      0.991  6  8   \n",
       "394    7.242      0.433   11.428      0.194    0.491      1.006  6  8   \n",
       "\n",
       "     Product_RTTA  Product_RTT12A  Product_RTT23  Product_RTT13A  \n",
       "0      112.702990       10.211379      14.469507       85.967193  \n",
       "1       80.723608        7.723269      10.441548       80.804412  \n",
       "2       83.649729        8.929305      10.820040       72.424008  \n",
       "3       88.938013        9.160368      11.592546       74.487448  \n",
       "4       77.335115        7.933434      10.118424       74.503964  \n",
       "..            ...             ...            ...             ...  \n",
       "390     40.154764       81.781596       5.342571        3.690356  \n",
       "391     36.259386       80.219880       4.953468        3.308640  \n",
       "392     42.365232       79.934400       5.787600        3.879600  \n",
       "393     36.954976       81.940080       5.048494        3.301320  \n",
       "394     40.635934       82.761576       5.611148        3.555822  \n",
       "\n",
       "[14005 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_name={'AP1RTT':'AP1RTTA','AP1STDEV':'AP1STDEVA','AP2RTT':'AP2RTTA','AP2STDEV':'AP2STDEVA','AP3RTT':'AP3RTTA','AP3STDEV':'AP3STDEVA','Product_RTT':'Product_RTTA','Product_RTT12':'Product_RTT12A','Product_RTT13':'Product_RTT13A','square_RTT1':'square_RTT1A','square_RTT2':'square_RTT2A','square_RTT3':'square_RTT3A'}\n",
    "df1.rename(columns=new_name,inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df627df",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTT':'AP1RTTB','AP1STDEV':'AP1STDEVB','AP2RTT':'AP2RTTB','AP2STDEV':'AP2STDEVB','AP3RTT':'AP3RTTB','AP3STDEV':'AP3STDEVB','Product_RTT':'Product_RTTB','Product_RTT12':'Product_RTT12B','Product_RTT13':'Product_RTT13B','square_RTT1':'square_RTT1B','square_RTT2':'square_RTT2B','square_RTT3':'square_RTT3B'}\n",
    "df2.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8d0102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by x and y values, and calculate the mean of each group\n",
    "groupedd1 = df1.groupby(['x', 'y']).mean()\n",
    "groupedd2 = df2.groupby(['x', 'y']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5d47f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'x' and 'y', and calculate the minimum, maximum, 25th, 50th, and 75th percentiles for each column for local feature extractions\n",
    "\n",
    "grouped = df1.groupby(['x', 'y']).agg(['min', 'max', lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=50), lambda x: np.percentile(x, q=75)])\n",
    "\n",
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped_mean = df1.groupby(['x', 'y']).mean()\n",
    "grouped = pd.concat([grouped, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "grouped = grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1178b3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'x' and 'y', and calculate the minimum, maximum, 25th, 50th, and 75th percentiles for each column for local feature extractions\n",
    "\n",
    "groupedd = df2.groupby(['x', 'y']).agg(['min', 'max', lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=50), lambda x: np.percentile(x, q=75)])\n",
    "\n",
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped_mean = df2.groupby(['x', 'y']).mean()\n",
    "groupedd = pd.concat([groupedd, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "groupedd.columns = ['_'.join(col).strip() for col in groupedd.columns.values]\n",
    "groupedd = groupedd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b0d87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTTA_<lambda_0>': 'AP1RTTA_25','AP2RTTA_<lambda_0>': 'AP2RTTA_25','AP3RTTA_<lambda_0>': 'AP3RTTA_25',\n",
    "          'AP1RTTA_<lambda_1>': 'AP1RTTA_50','AP2RTTA_<lambda_1>': 'AP2RTTA_50','AP3RTTA_<lambda_1>': 'AP3RTTA_50',\n",
    "          'AP1RTTA_<lambda_2>': 'AP1RTTA_75','AP2RTTA_<lambda_2>': 'AP2RTTA_75','AP3RTTA_<lambda_2>': 'AP3RTTA_75',\n",
    "           \n",
    "             \n",
    "          'AP1STDEVA_<lambda_0>':'AP1STDEVA_25','AP2STDEVA_<lambda_0>':'AP2STDEVA_25','AP3STDEVA_<lambda_0>':'AP3STDEVA_25',\n",
    "          'AP1STDEVA_<lambda_1>':'AP1STDEVA_50','AP2STDEVA_<lambda_1>':'AP2STDEVA_50','AP3STDEVA_<lambda_1>':'AP3STDEVA_50',\n",
    "          'AP1STDEVA_<lambda_2>':'AP1STDEVA_75','AP2STDEVA_<lambda_2>':'AP2STDEVA_75','AP3STDEVA_<lambda_2>':'AP3STDEVA_75',\n",
    "          'A_P_1_R_T_T_A':'AP1RTTA_MEAN','A_P_2_R_T_T_A':'AP2RTTA_MEAN','A_P_3_R_T_T_A_A':'AP3RTTA_MEAN',\n",
    "          'A_P_1_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_2_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_3_S_T_D_E_V_A':'AP1STDEVA_MEAN'}\n",
    "grouped.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "706766ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTTB_<lambda_0>': 'AP1RTTB_25','AP2RTTB_<lambda_0>': 'AP2RTTB_25','AP3RTTB_<lambda_0>': 'AP3RTTB_25',\n",
    "          'AP1RTTB_<lambda_1>': 'AP1RTTB_50','AP2RTTB_<lambda_1>': 'AP2RTTB_50','AP3RTTB_<lambda_1>': 'AP3RTTB_50',\n",
    "          'AP1RTTB_<lambda_2>': 'AP1RTTB_75','AP2RTTB_<lambda_2>': 'AP2RTTB_75','AP3RTTB_<lambda_2>': 'AP3RTTB_75',\n",
    "           \n",
    "             \n",
    "          'AP1STDEVB_<lambda_0>':'AP1STDEVB_25','AP2STDEVB_<lambda_0>':'AP2STDEVB_25','AP3STDEVB_<lambda_0>':'AP3STDEVB_25',\n",
    "          'AP1STDEVB_<lambda_1>':'AP1STDEVB_50','AP2STDEVB_<lambda_1>':'AP2STDEVB_50','AP3STDEVB_<lambda_1>':'AP3STDEVB_50',\n",
    "          'AP1STDEVB_<lambda_2>':'AP1STDEVB_75','AP2STDEVB_<lambda_2>':'AP2STDEVB_75','AP3STDEVB_<lambda_2>':'AP3STDEVB_75',\n",
    "          'A_P_1_R_T_T_A':'AP1RTTA_MEAN','A_P_2_R_T_T_A':'AP2RTTA_MEAN','A_P_3_R_T_T_A_A':'AP3RTTA_MEAN',\n",
    "          'A_P_1_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_2_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_3_S_T_D_E_V_A':'AP1STDEVA_MEAN'}\n",
    "groupedd.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "272580d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP1RTTA_min</th>\n",
       "      <th>AP1RTTA_max</th>\n",
       "      <th>AP1RTTA_25</th>\n",
       "      <th>AP1RTTA_50</th>\n",
       "      <th>AP1RTTA_75</th>\n",
       "      <th>AP1STDEVA_min</th>\n",
       "      <th>AP1STDEVA_max</th>\n",
       "      <th>AP1STDEVA_25</th>\n",
       "      <th>AP1STDEVA_50</th>\n",
       "      <th>AP1STDEVA_75</th>\n",
       "      <th>...</th>\n",
       "      <th>A_P_1_R_T_T_B</th>\n",
       "      <th>A_P_1_S_T_D_E_V_B</th>\n",
       "      <th>A_P_2_R_T_T_B</th>\n",
       "      <th>A_P_2_S_T_D_E_V_B</th>\n",
       "      <th>A_P_3_R_T_T_B</th>\n",
       "      <th>A_P_3_S_T_D_E_V_B</th>\n",
       "      <th>P_r_o_d_u_c_t___R_T_T_B</th>\n",
       "      <th>P_r_o_d_u_c_t___R_T_T_1_2_B</th>\n",
       "      <th>P_r_o_d_u_c_t___R_T_T_2_3</th>\n",
       "      <th>P_r_o_d_u_c_t___R_T_T_1_3_B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.291</td>\n",
       "      <td>9.781</td>\n",
       "      <td>7.5550</td>\n",
       "      <td>7.613</td>\n",
       "      <td>7.7010</td>\n",
       "      <td>0.083</td>\n",
       "      <td>2.875</td>\n",
       "      <td>0.93850</td>\n",
       "      <td>1.9780</td>\n",
       "      <td>2.22000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.701247</td>\n",
       "      <td>1.284562</td>\n",
       "      <td>1.045425</td>\n",
       "      <td>0.141963</td>\n",
       "      <td>10.495666</td>\n",
       "      <td>1.765401</td>\n",
       "      <td>84.501124</td>\n",
       "      <td>8.051205</td>\n",
       "      <td>10.972355</td>\n",
       "      <td>80.828673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.499</td>\n",
       "      <td>13.648</td>\n",
       "      <td>7.2030</td>\n",
       "      <td>7.291</td>\n",
       "      <td>7.3790</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.465</td>\n",
       "      <td>1.26700</td>\n",
       "      <td>1.5080</td>\n",
       "      <td>2.16500</td>\n",
       "      <td>...</td>\n",
       "      <td>7.319855</td>\n",
       "      <td>1.598138</td>\n",
       "      <td>2.084158</td>\n",
       "      <td>1.720347</td>\n",
       "      <td>9.855404</td>\n",
       "      <td>0.802764</td>\n",
       "      <td>150.350479</td>\n",
       "      <td>15.256179</td>\n",
       "      <td>20.540028</td>\n",
       "      <td>72.138262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.918</td>\n",
       "      <td>8.467</td>\n",
       "      <td>5.4060</td>\n",
       "      <td>5.484</td>\n",
       "      <td>5.6020</td>\n",
       "      <td>0.129</td>\n",
       "      <td>2.676</td>\n",
       "      <td>0.95650</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>1.21650</td>\n",
       "      <td>...</td>\n",
       "      <td>5.561764</td>\n",
       "      <td>1.070146</td>\n",
       "      <td>2.954241</td>\n",
       "      <td>0.302136</td>\n",
       "      <td>8.922131</td>\n",
       "      <td>0.415342</td>\n",
       "      <td>146.645583</td>\n",
       "      <td>16.434603</td>\n",
       "      <td>26.360476</td>\n",
       "      <td>49.623094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.492</td>\n",
       "      <td>6.827</td>\n",
       "      <td>5.0640</td>\n",
       "      <td>5.182</td>\n",
       "      <td>5.3280</td>\n",
       "      <td>0.058</td>\n",
       "      <td>2.696</td>\n",
       "      <td>0.18400</td>\n",
       "      <td>0.2660</td>\n",
       "      <td>0.40900</td>\n",
       "      <td>...</td>\n",
       "      <td>5.259823</td>\n",
       "      <td>0.611491</td>\n",
       "      <td>2.998302</td>\n",
       "      <td>0.281244</td>\n",
       "      <td>11.299793</td>\n",
       "      <td>0.164010</td>\n",
       "      <td>178.183766</td>\n",
       "      <td>15.769445</td>\n",
       "      <td>33.879835</td>\n",
       "      <td>59.432941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.491</td>\n",
       "      <td>4.303</td>\n",
       "      <td>3.6085</td>\n",
       "      <td>3.775</td>\n",
       "      <td>3.9810</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.977</td>\n",
       "      <td>0.16550</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.29150</td>\n",
       "      <td>...</td>\n",
       "      <td>3.841902</td>\n",
       "      <td>0.868242</td>\n",
       "      <td>4.311887</td>\n",
       "      <td>0.598031</td>\n",
       "      <td>7.111352</td>\n",
       "      <td>1.792018</td>\n",
       "      <td>117.829467</td>\n",
       "      <td>16.570329</td>\n",
       "      <td>30.662568</td>\n",
       "      <td>27.319950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.789</td>\n",
       "      <td>6.148</td>\n",
       "      <td>4.6250</td>\n",
       "      <td>4.929</td>\n",
       "      <td>5.6210</td>\n",
       "      <td>0.052</td>\n",
       "      <td>2.680</td>\n",
       "      <td>0.18600</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>0.66500</td>\n",
       "      <td>...</td>\n",
       "      <td>4.858888</td>\n",
       "      <td>0.526955</td>\n",
       "      <td>7.449432</td>\n",
       "      <td>0.138517</td>\n",
       "      <td>6.057580</td>\n",
       "      <td>0.280483</td>\n",
       "      <td>219.305192</td>\n",
       "      <td>36.191185</td>\n",
       "      <td>45.124512</td>\n",
       "      <td>29.443773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.178</td>\n",
       "      <td>3.624</td>\n",
       "      <td>2.5940</td>\n",
       "      <td>2.672</td>\n",
       "      <td>2.7110</td>\n",
       "      <td>0.057</td>\n",
       "      <td>2.625</td>\n",
       "      <td>2.15625</td>\n",
       "      <td>2.2795</td>\n",
       "      <td>2.38800</td>\n",
       "      <td>...</td>\n",
       "      <td>2.671067</td>\n",
       "      <td>2.285251</td>\n",
       "      <td>6.636018</td>\n",
       "      <td>1.623611</td>\n",
       "      <td>7.180249</td>\n",
       "      <td>0.925377</td>\n",
       "      <td>127.267301</td>\n",
       "      <td>17.724517</td>\n",
       "      <td>47.648455</td>\n",
       "      <td>19.178980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.900</td>\n",
       "      <td>1.695</td>\n",
       "      <td>1.0700</td>\n",
       "      <td>1.178</td>\n",
       "      <td>1.2560</td>\n",
       "      <td>0.053</td>\n",
       "      <td>1.978</td>\n",
       "      <td>0.13800</td>\n",
       "      <td>0.1750</td>\n",
       "      <td>0.23300</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179289</td>\n",
       "      <td>0.293636</td>\n",
       "      <td>10.096093</td>\n",
       "      <td>0.168584</td>\n",
       "      <td>5.669654</td>\n",
       "      <td>1.064750</td>\n",
       "      <td>67.503082</td>\n",
       "      <td>11.906871</td>\n",
       "      <td>57.241202</td>\n",
       "      <td>6.685706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.516</td>\n",
       "      <td>8.229</td>\n",
       "      <td>7.8670</td>\n",
       "      <td>7.945</td>\n",
       "      <td>8.0230</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.09300</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.18350</td>\n",
       "      <td>...</td>\n",
       "      <td>7.991153</td>\n",
       "      <td>0.173280</td>\n",
       "      <td>2.513745</td>\n",
       "      <td>0.851494</td>\n",
       "      <td>10.413860</td>\n",
       "      <td>0.758885</td>\n",
       "      <td>209.189829</td>\n",
       "      <td>20.087880</td>\n",
       "      <td>26.178898</td>\n",
       "      <td>83.214182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.266</td>\n",
       "      <td>8.414</td>\n",
       "      <td>6.4610</td>\n",
       "      <td>6.578</td>\n",
       "      <td>6.6560</td>\n",
       "      <td>0.841</td>\n",
       "      <td>2.412</td>\n",
       "      <td>1.76975</td>\n",
       "      <td>1.8515</td>\n",
       "      <td>1.96300</td>\n",
       "      <td>...</td>\n",
       "      <td>6.593670</td>\n",
       "      <td>1.863201</td>\n",
       "      <td>3.477179</td>\n",
       "      <td>0.184102</td>\n",
       "      <td>9.097593</td>\n",
       "      <td>1.571577</td>\n",
       "      <td>208.578083</td>\n",
       "      <td>22.929268</td>\n",
       "      <td>31.633872</td>\n",
       "      <td>59.980020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4.444</td>\n",
       "      <td>6.266</td>\n",
       "      <td>6.0310</td>\n",
       "      <td>6.070</td>\n",
       "      <td>6.1480</td>\n",
       "      <td>0.173</td>\n",
       "      <td>1.945</td>\n",
       "      <td>0.28100</td>\n",
       "      <td>0.3300</td>\n",
       "      <td>0.38500</td>\n",
       "      <td>...</td>\n",
       "      <td>5.991196</td>\n",
       "      <td>0.366653</td>\n",
       "      <td>3.772486</td>\n",
       "      <td>0.123984</td>\n",
       "      <td>7.663114</td>\n",
       "      <td>0.271956</td>\n",
       "      <td>173.193674</td>\n",
       "      <td>22.601107</td>\n",
       "      <td>28.908739</td>\n",
       "      <td>45.911292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.195</td>\n",
       "      <td>5.186</td>\n",
       "      <td>4.3510</td>\n",
       "      <td>4.391</td>\n",
       "      <td>4.4690</td>\n",
       "      <td>0.456</td>\n",
       "      <td>2.238</td>\n",
       "      <td>0.58300</td>\n",
       "      <td>0.6340</td>\n",
       "      <td>0.69400</td>\n",
       "      <td>...</td>\n",
       "      <td>4.435881</td>\n",
       "      <td>0.702222</td>\n",
       "      <td>5.427796</td>\n",
       "      <td>0.829842</td>\n",
       "      <td>7.626401</td>\n",
       "      <td>0.950328</td>\n",
       "      <td>183.633350</td>\n",
       "      <td>24.078188</td>\n",
       "      <td>41.394540</td>\n",
       "      <td>33.830456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.134</td>\n",
       "      <td>4.742</td>\n",
       "      <td>4.3120</td>\n",
       "      <td>4.430</td>\n",
       "      <td>4.5080</td>\n",
       "      <td>0.063</td>\n",
       "      <td>4.204</td>\n",
       "      <td>0.22250</td>\n",
       "      <td>0.2740</td>\n",
       "      <td>0.34000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.284173</td>\n",
       "      <td>0.476914</td>\n",
       "      <td>5.046579</td>\n",
       "      <td>1.167715</td>\n",
       "      <td>6.846530</td>\n",
       "      <td>1.073830</td>\n",
       "      <td>148.005120</td>\n",
       "      <td>21.616111</td>\n",
       "      <td>34.551484</td>\n",
       "      <td>29.333792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.516</td>\n",
       "      <td>4.795</td>\n",
       "      <td>3.5700</td>\n",
       "      <td>3.648</td>\n",
       "      <td>3.7270</td>\n",
       "      <td>0.196</td>\n",
       "      <td>1.567</td>\n",
       "      <td>0.58000</td>\n",
       "      <td>0.6940</td>\n",
       "      <td>0.92600</td>\n",
       "      <td>...</td>\n",
       "      <td>3.696801</td>\n",
       "      <td>0.718538</td>\n",
       "      <td>5.776027</td>\n",
       "      <td>0.579422</td>\n",
       "      <td>4.426642</td>\n",
       "      <td>0.739459</td>\n",
       "      <td>94.512869</td>\n",
       "      <td>21.351754</td>\n",
       "      <td>25.567975</td>\n",
       "      <td>16.364035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3.453</td>\n",
       "      <td>5.303</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.039</td>\n",
       "      <td>4.1170</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.291</td>\n",
       "      <td>0.67700</td>\n",
       "      <td>0.7280</td>\n",
       "      <td>0.82300</td>\n",
       "      <td>...</td>\n",
       "      <td>4.078306</td>\n",
       "      <td>0.768500</td>\n",
       "      <td>6.628178</td>\n",
       "      <td>0.514992</td>\n",
       "      <td>5.811444</td>\n",
       "      <td>0.624411</td>\n",
       "      <td>157.093991</td>\n",
       "      <td>27.031638</td>\n",
       "      <td>38.519394</td>\n",
       "      <td>23.700957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.076</td>\n",
       "      <td>5.885</td>\n",
       "      <td>4.2440</td>\n",
       "      <td>4.332</td>\n",
       "      <td>4.4200</td>\n",
       "      <td>0.098</td>\n",
       "      <td>2.181</td>\n",
       "      <td>0.22100</td>\n",
       "      <td>0.2730</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.443587</td>\n",
       "      <td>0.252731</td>\n",
       "      <td>8.893570</td>\n",
       "      <td>0.267026</td>\n",
       "      <td>4.554344</td>\n",
       "      <td>1.040989</td>\n",
       "      <td>179.950303</td>\n",
       "      <td>39.511292</td>\n",
       "      <td>40.504470</td>\n",
       "      <td>20.237850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7.712</td>\n",
       "      <td>10.865</td>\n",
       "      <td>8.0230</td>\n",
       "      <td>8.141</td>\n",
       "      <td>8.1990</td>\n",
       "      <td>0.444</td>\n",
       "      <td>3.063</td>\n",
       "      <td>1.20600</td>\n",
       "      <td>1.2940</td>\n",
       "      <td>1.36700</td>\n",
       "      <td>...</td>\n",
       "      <td>8.135619</td>\n",
       "      <td>1.370246</td>\n",
       "      <td>3.613415</td>\n",
       "      <td>1.179244</td>\n",
       "      <td>9.679940</td>\n",
       "      <td>0.805656</td>\n",
       "      <td>284.556338</td>\n",
       "      <td>29.398128</td>\n",
       "      <td>34.976033</td>\n",
       "      <td>78.751485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6.202</td>\n",
       "      <td>8.272</td>\n",
       "      <td>7.2030</td>\n",
       "      <td>7.281</td>\n",
       "      <td>7.3980</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.984</td>\n",
       "      <td>1.06975</td>\n",
       "      <td>1.1530</td>\n",
       "      <td>1.23600</td>\n",
       "      <td>...</td>\n",
       "      <td>7.266462</td>\n",
       "      <td>1.195492</td>\n",
       "      <td>3.717670</td>\n",
       "      <td>0.340747</td>\n",
       "      <td>8.455948</td>\n",
       "      <td>0.518209</td>\n",
       "      <td>228.431005</td>\n",
       "      <td>27.013915</td>\n",
       "      <td>31.436538</td>\n",
       "      <td>61.445338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.364</td>\n",
       "      <td>14.234</td>\n",
       "      <td>7.1640</td>\n",
       "      <td>7.281</td>\n",
       "      <td>7.3980</td>\n",
       "      <td>0.057</td>\n",
       "      <td>3.289</td>\n",
       "      <td>0.14200</td>\n",
       "      <td>0.2270</td>\n",
       "      <td>0.44200</td>\n",
       "      <td>...</td>\n",
       "      <td>7.360543</td>\n",
       "      <td>0.470111</td>\n",
       "      <td>4.049911</td>\n",
       "      <td>0.418374</td>\n",
       "      <td>7.791006</td>\n",
       "      <td>0.205618</td>\n",
       "      <td>232.239060</td>\n",
       "      <td>29.809048</td>\n",
       "      <td>31.553261</td>\n",
       "      <td>57.344499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.803</td>\n",
       "      <td>5.885</td>\n",
       "      <td>4.9180</td>\n",
       "      <td>5.006</td>\n",
       "      <td>5.0940</td>\n",
       "      <td>0.526</td>\n",
       "      <td>3.400</td>\n",
       "      <td>0.73850</td>\n",
       "      <td>0.8210</td>\n",
       "      <td>0.90350</td>\n",
       "      <td>...</td>\n",
       "      <td>4.966472</td>\n",
       "      <td>0.916813</td>\n",
       "      <td>5.063435</td>\n",
       "      <td>0.889475</td>\n",
       "      <td>6.707043</td>\n",
       "      <td>2.137917</td>\n",
       "      <td>168.858753</td>\n",
       "      <td>25.147993</td>\n",
       "      <td>33.993675</td>\n",
       "      <td>33.315384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.670</td>\n",
       "      <td>4.225</td>\n",
       "      <td>3.7170</td>\n",
       "      <td>3.775</td>\n",
       "      <td>3.8340</td>\n",
       "      <td>0.106</td>\n",
       "      <td>3.341</td>\n",
       "      <td>0.75925</td>\n",
       "      <td>0.9040</td>\n",
       "      <td>1.17375</td>\n",
       "      <td>...</td>\n",
       "      <td>3.641076</td>\n",
       "      <td>0.825395</td>\n",
       "      <td>8.723374</td>\n",
       "      <td>1.653500</td>\n",
       "      <td>4.555558</td>\n",
       "      <td>0.355508</td>\n",
       "      <td>144.704609</td>\n",
       "      <td>31.761361</td>\n",
       "      <td>39.739908</td>\n",
       "      <td>16.588691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7.712</td>\n",
       "      <td>16.754</td>\n",
       "      <td>9.1560</td>\n",
       "      <td>9.288</td>\n",
       "      <td>10.2395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.385</td>\n",
       "      <td>0.17600</td>\n",
       "      <td>0.4490</td>\n",
       "      <td>1.70400</td>\n",
       "      <td>...</td>\n",
       "      <td>9.316530</td>\n",
       "      <td>0.724311</td>\n",
       "      <td>5.758195</td>\n",
       "      <td>1.994493</td>\n",
       "      <td>11.299842</td>\n",
       "      <td>1.630470</td>\n",
       "      <td>606.713807</td>\n",
       "      <td>53.651169</td>\n",
       "      <td>65.077842</td>\n",
       "      <td>105.338581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.164</td>\n",
       "      <td>11.187</td>\n",
       "      <td>7.5160</td>\n",
       "      <td>7.594</td>\n",
       "      <td>7.6720</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.551</td>\n",
       "      <td>0.95800</td>\n",
       "      <td>1.0960</td>\n",
       "      <td>1.29300</td>\n",
       "      <td>...</td>\n",
       "      <td>7.631718</td>\n",
       "      <td>1.174695</td>\n",
       "      <td>4.838801</td>\n",
       "      <td>0.798354</td>\n",
       "      <td>7.887724</td>\n",
       "      <td>0.548664</td>\n",
       "      <td>291.267644</td>\n",
       "      <td>36.928914</td>\n",
       "      <td>38.166387</td>\n",
       "      <td>60.194439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.772</td>\n",
       "      <td>8.316</td>\n",
       "      <td>6.9390</td>\n",
       "      <td>7.027</td>\n",
       "      <td>7.1440</td>\n",
       "      <td>0.053</td>\n",
       "      <td>3.135</td>\n",
       "      <td>0.98200</td>\n",
       "      <td>1.1280</td>\n",
       "      <td>1.36650</td>\n",
       "      <td>...</td>\n",
       "      <td>6.888355</td>\n",
       "      <td>1.417582</td>\n",
       "      <td>5.023658</td>\n",
       "      <td>1.818303</td>\n",
       "      <td>9.576689</td>\n",
       "      <td>0.204503</td>\n",
       "      <td>331.411231</td>\n",
       "      <td>34.605986</td>\n",
       "      <td>48.109547</td>\n",
       "      <td>65.968388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>6.578</td>\n",
       "      <td>10.772</td>\n",
       "      <td>6.8520</td>\n",
       "      <td>6.930</td>\n",
       "      <td>7.0080</td>\n",
       "      <td>0.146</td>\n",
       "      <td>3.841</td>\n",
       "      <td>1.75000</td>\n",
       "      <td>1.8560</td>\n",
       "      <td>1.95700</td>\n",
       "      <td>...</td>\n",
       "      <td>7.176985</td>\n",
       "      <td>1.955949</td>\n",
       "      <td>7.687108</td>\n",
       "      <td>0.646799</td>\n",
       "      <td>7.536028</td>\n",
       "      <td>0.311985</td>\n",
       "      <td>415.805938</td>\n",
       "      <td>55.169271</td>\n",
       "      <td>57.943407</td>\n",
       "      <td>54.081431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4.060</td>\n",
       "      <td>11.755</td>\n",
       "      <td>5.2110</td>\n",
       "      <td>5.299</td>\n",
       "      <td>5.3670</td>\n",
       "      <td>0.085</td>\n",
       "      <td>4.276</td>\n",
       "      <td>0.35400</td>\n",
       "      <td>0.4840</td>\n",
       "      <td>0.76950</td>\n",
       "      <td>...</td>\n",
       "      <td>5.289499</td>\n",
       "      <td>0.647979</td>\n",
       "      <td>6.714875</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>5.445504</td>\n",
       "      <td>1.113446</td>\n",
       "      <td>193.400901</td>\n",
       "      <td>35.520183</td>\n",
       "      <td>36.563007</td>\n",
       "      <td>28.802768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.152</td>\n",
       "      <td>6.324</td>\n",
       "      <td>5.5630</td>\n",
       "      <td>5.650</td>\n",
       "      <td>5.8460</td>\n",
       "      <td>0.086</td>\n",
       "      <td>2.425</td>\n",
       "      <td>1.01200</td>\n",
       "      <td>1.1370</td>\n",
       "      <td>1.23900</td>\n",
       "      <td>...</td>\n",
       "      <td>5.733540</td>\n",
       "      <td>1.174977</td>\n",
       "      <td>6.922496</td>\n",
       "      <td>0.311783</td>\n",
       "      <td>3.556346</td>\n",
       "      <td>0.813111</td>\n",
       "      <td>141.145934</td>\n",
       "      <td>39.688458</td>\n",
       "      <td>24.618752</td>\n",
       "      <td>20.390494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.684</td>\n",
       "      <td>6.969</td>\n",
       "      <td>6.3830</td>\n",
       "      <td>6.529</td>\n",
       "      <td>6.6460</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.10800</td>\n",
       "      <td>0.1480</td>\n",
       "      <td>0.26400</td>\n",
       "      <td>...</td>\n",
       "      <td>6.343611</td>\n",
       "      <td>0.342574</td>\n",
       "      <td>7.792909</td>\n",
       "      <td>0.271413</td>\n",
       "      <td>3.357062</td>\n",
       "      <td>0.331555</td>\n",
       "      <td>165.945137</td>\n",
       "      <td>49.434571</td>\n",
       "      <td>26.161287</td>\n",
       "      <td>21.294621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.859</td>\n",
       "      <td>11.816</td>\n",
       "      <td>7.4035</td>\n",
       "      <td>7.555</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.365</td>\n",
       "      <td>0.89900</td>\n",
       "      <td>1.0640</td>\n",
       "      <td>1.19850</td>\n",
       "      <td>...</td>\n",
       "      <td>7.609945</td>\n",
       "      <td>1.023094</td>\n",
       "      <td>11.253671</td>\n",
       "      <td>0.292603</td>\n",
       "      <td>3.889475</td>\n",
       "      <td>0.646008</td>\n",
       "      <td>333.221078</td>\n",
       "      <td>85.670931</td>\n",
       "      <td>43.772010</td>\n",
       "      <td>29.598461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>9.312</td>\n",
       "      <td>12.048</td>\n",
       "      <td>11.3050</td>\n",
       "      <td>11.393</td>\n",
       "      <td>11.5100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.263</td>\n",
       "      <td>0.45600</td>\n",
       "      <td>0.5575</td>\n",
       "      <td>0.98950</td>\n",
       "      <td>...</td>\n",
       "      <td>11.440885</td>\n",
       "      <td>0.705821</td>\n",
       "      <td>5.799641</td>\n",
       "      <td>1.094408</td>\n",
       "      <td>8.492251</td>\n",
       "      <td>0.573785</td>\n",
       "      <td>563.763462</td>\n",
       "      <td>66.369916</td>\n",
       "      <td>49.265219</td>\n",
       "      <td>97.156259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>7.100</td>\n",
       "      <td>10.211</td>\n",
       "      <td>9.2730</td>\n",
       "      <td>9.430</td>\n",
       "      <td>9.4880</td>\n",
       "      <td>0.234</td>\n",
       "      <td>2.606</td>\n",
       "      <td>0.75300</td>\n",
       "      <td>0.7930</td>\n",
       "      <td>0.84000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.314909</td>\n",
       "      <td>0.816597</td>\n",
       "      <td>5.544236</td>\n",
       "      <td>0.214925</td>\n",
       "      <td>9.497439</td>\n",
       "      <td>0.891429</td>\n",
       "      <td>490.283285</td>\n",
       "      <td>51.643667</td>\n",
       "      <td>52.655095</td>\n",
       "      <td>88.433425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>7.097</td>\n",
       "      <td>10.296</td>\n",
       "      <td>9.0490</td>\n",
       "      <td>9.166</td>\n",
       "      <td>9.2540</td>\n",
       "      <td>0.138</td>\n",
       "      <td>2.154</td>\n",
       "      <td>0.58200</td>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.71100</td>\n",
       "      <td>...</td>\n",
       "      <td>9.274397</td>\n",
       "      <td>0.618753</td>\n",
       "      <td>5.817848</td>\n",
       "      <td>1.605957</td>\n",
       "      <td>7.237160</td>\n",
       "      <td>1.708095</td>\n",
       "      <td>390.518487</td>\n",
       "      <td>53.958978</td>\n",
       "      <td>42.105305</td>\n",
       "      <td>67.120811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.918</td>\n",
       "      <td>7.565</td>\n",
       "      <td>6.0700</td>\n",
       "      <td>6.578</td>\n",
       "      <td>7.0860</td>\n",
       "      <td>0.067</td>\n",
       "      <td>3.159</td>\n",
       "      <td>0.26400</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.72100</td>\n",
       "      <td>...</td>\n",
       "      <td>6.151108</td>\n",
       "      <td>0.737088</td>\n",
       "      <td>7.890436</td>\n",
       "      <td>0.745549</td>\n",
       "      <td>2.730353</td>\n",
       "      <td>0.200416</td>\n",
       "      <td>132.571776</td>\n",
       "      <td>48.555863</td>\n",
       "      <td>21.544257</td>\n",
       "      <td>16.793847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.602</td>\n",
       "      <td>9.313</td>\n",
       "      <td>8.2970</td>\n",
       "      <td>8.521</td>\n",
       "      <td>8.6880</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.312</td>\n",
       "      <td>0.28900</td>\n",
       "      <td>0.3960</td>\n",
       "      <td>0.49500</td>\n",
       "      <td>...</td>\n",
       "      <td>8.382122</td>\n",
       "      <td>0.474463</td>\n",
       "      <td>9.475679</td>\n",
       "      <td>0.147662</td>\n",
       "      <td>1.721144</td>\n",
       "      <td>1.417366</td>\n",
       "      <td>136.691918</td>\n",
       "      <td>79.426609</td>\n",
       "      <td>16.307616</td>\n",
       "      <td>14.426766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9.371</td>\n",
       "      <td>12.359</td>\n",
       "      <td>9.9860</td>\n",
       "      <td>10.250</td>\n",
       "      <td>10.5430</td>\n",
       "      <td>0.124</td>\n",
       "      <td>2.368</td>\n",
       "      <td>0.90200</td>\n",
       "      <td>1.0420</td>\n",
       "      <td>1.15100</td>\n",
       "      <td>...</td>\n",
       "      <td>10.039902</td>\n",
       "      <td>1.116149</td>\n",
       "      <td>5.958733</td>\n",
       "      <td>0.144283</td>\n",
       "      <td>9.413162</td>\n",
       "      <td>0.136864</td>\n",
       "      <td>563.154420</td>\n",
       "      <td>59.824874</td>\n",
       "      <td>56.090505</td>\n",
       "      <td>94.509465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.177</td>\n",
       "      <td>11.031</td>\n",
       "      <td>10.3670</td>\n",
       "      <td>10.396</td>\n",
       "      <td>10.4550</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.568</td>\n",
       "      <td>0.26200</td>\n",
       "      <td>0.2970</td>\n",
       "      <td>0.34000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.429995</td>\n",
       "      <td>0.424747</td>\n",
       "      <td>7.118130</td>\n",
       "      <td>0.510474</td>\n",
       "      <td>9.894398</td>\n",
       "      <td>0.443328</td>\n",
       "      <td>734.560273</td>\n",
       "      <td>74.240654</td>\n",
       "      <td>70.428844</td>\n",
       "      <td>103.198779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>8.922</td>\n",
       "      <td>9.704</td>\n",
       "      <td>9.2780</td>\n",
       "      <td>9.371</td>\n",
       "      <td>9.4690</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.189</td>\n",
       "      <td>0.12850</td>\n",
       "      <td>0.2080</td>\n",
       "      <td>0.43600</td>\n",
       "      <td>...</td>\n",
       "      <td>9.258583</td>\n",
       "      <td>0.286670</td>\n",
       "      <td>9.062368</td>\n",
       "      <td>0.814729</td>\n",
       "      <td>7.160629</td>\n",
       "      <td>0.390941</td>\n",
       "      <td>600.908796</td>\n",
       "      <td>83.910441</td>\n",
       "      <td>64.898712</td>\n",
       "      <td>66.296987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.484</td>\n",
       "      <td>7.408</td>\n",
       "      <td>5.6410</td>\n",
       "      <td>5.719</td>\n",
       "      <td>5.7970</td>\n",
       "      <td>0.306</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.83100</td>\n",
       "      <td>0.8920</td>\n",
       "      <td>0.95200</td>\n",
       "      <td>...</td>\n",
       "      <td>5.726580</td>\n",
       "      <td>0.893827</td>\n",
       "      <td>9.739361</td>\n",
       "      <td>0.799677</td>\n",
       "      <td>1.911560</td>\n",
       "      <td>0.142254</td>\n",
       "      <td>106.607213</td>\n",
       "      <td>55.773370</td>\n",
       "      <td>18.617340</td>\n",
       "      <td>10.946011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6.006</td>\n",
       "      <td>8.375</td>\n",
       "      <td>7.1640</td>\n",
       "      <td>7.281</td>\n",
       "      <td>7.3590</td>\n",
       "      <td>0.085</td>\n",
       "      <td>2.070</td>\n",
       "      <td>0.27450</td>\n",
       "      <td>0.3640</td>\n",
       "      <td>0.45700</td>\n",
       "      <td>...</td>\n",
       "      <td>7.150149</td>\n",
       "      <td>0.495289</td>\n",
       "      <td>11.171871</td>\n",
       "      <td>0.351382</td>\n",
       "      <td>0.476068</td>\n",
       "      <td>0.984149</td>\n",
       "      <td>38.026342</td>\n",
       "      <td>79.875620</td>\n",
       "      <td>5.319558</td>\n",
       "      <td>3.403318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    AP1RTTA_min  AP1RTTA_max  AP1RTTA_25  AP1RTTA_50  AP1RTTA_75  \\\n",
       "0         7.291        9.781      7.5550       7.613      7.7010   \n",
       "1         5.499       13.648      7.2030       7.291      7.3790   \n",
       "2         4.918        8.467      5.4060       5.484      5.6020   \n",
       "3         3.492        6.827      5.0640       5.182      5.3280   \n",
       "4         2.491        4.303      3.6085       3.775      3.9810   \n",
       "5         2.789        6.148      4.6250       4.929      5.6210   \n",
       "6         2.178        3.624      2.5940       2.672      2.7110   \n",
       "7         0.900        1.695      1.0700       1.178      1.2560   \n",
       "8         7.516        8.229      7.8670       7.945      8.0230   \n",
       "9         6.266        8.414      6.4610       6.578      6.6560   \n",
       "10        4.444        6.266      6.0310       6.070      6.1480   \n",
       "11        4.195        5.186      4.3510       4.391      4.4690   \n",
       "12        0.134        4.742      4.3120       4.430      4.5080   \n",
       "13        2.516        4.795      3.5700       3.648      3.7270   \n",
       "14        3.453        5.303      4.0000       4.039      4.1170   \n",
       "15        2.076        5.885      4.2440       4.332      4.4200   \n",
       "16        7.712       10.865      8.0230       8.141      8.1990   \n",
       "17        6.202        8.272      7.2030       7.281      7.3980   \n",
       "18        6.364       14.234      7.1640       7.281      7.3980   \n",
       "19        2.803        5.885      4.9180       5.006      5.0940   \n",
       "20        1.670        4.225      3.7170       3.775      3.8340   \n",
       "21        7.712       16.754      9.1560       9.288     10.2395   \n",
       "22        7.164       11.187      7.5160       7.594      7.6720   \n",
       "23        5.772        8.316      6.9390       7.027      7.1440   \n",
       "24        6.578       10.772      6.8520       6.930      7.0080   \n",
       "25        4.060       11.755      5.2110       5.299      5.3670   \n",
       "26        5.152        6.324      5.5630       5.650      5.8460   \n",
       "27        4.684        6.969      6.3830       6.529      6.6460   \n",
       "28        4.859       11.816      7.4035       7.555      7.7500   \n",
       "29        9.312       12.048     11.3050      11.393     11.5100   \n",
       "30        7.100       10.211      9.2730       9.430      9.4880   \n",
       "31        7.097       10.296      9.0490       9.166      9.2540   \n",
       "32        4.918        7.565      6.0700       6.578      7.0860   \n",
       "33        5.602        9.313      8.2970       8.521      8.6880   \n",
       "34        9.371       12.359      9.9860      10.250     10.5430   \n",
       "35        9.177       11.031     10.3670      10.396     10.4550   \n",
       "36        8.922        9.704      9.2780       9.371      9.4690   \n",
       "37        5.484        7.408      5.6410       5.719      5.7970   \n",
       "38        6.006        8.375      7.1640       7.281      7.3590   \n",
       "\n",
       "    AP1STDEVA_min  AP1STDEVA_max  AP1STDEVA_25  AP1STDEVA_50  AP1STDEVA_75  \\\n",
       "0           0.083          2.875       0.93850        1.9780       2.22000   \n",
       "1           0.000          4.465       1.26700        1.5080       2.16500   \n",
       "2           0.129          2.676       0.95650        1.0620       1.21650   \n",
       "3           0.058          2.696       0.18400        0.2660       0.40900   \n",
       "4           0.080          1.977       0.16550        0.2150       0.29150   \n",
       "5           0.052          2.680       0.18600        0.2670       0.66500   \n",
       "6           0.057          2.625       2.15625        2.2795       2.38800   \n",
       "7           0.053          1.978       0.13800        0.1750       0.23300   \n",
       "8           0.000          0.685       0.09300        0.1255       0.18350   \n",
       "9           0.841          2.412       1.76975        1.8515       1.96300   \n",
       "10          0.173          1.945       0.28100        0.3300       0.38500   \n",
       "11          0.456          2.238       0.58300        0.6340       0.69400   \n",
       "12          0.063          4.204       0.22250        0.2740       0.34000   \n",
       "13          0.196          1.567       0.58000        0.6940       0.92600   \n",
       "14          0.062          1.291       0.67700        0.7280       0.82300   \n",
       "15          0.098          2.181       0.22100        0.2730       0.33000   \n",
       "16          0.444          3.063       1.20600        1.2940       1.36700   \n",
       "17          0.106          1.984       1.06975        1.1530       1.23600   \n",
       "18          0.057          3.289       0.14200        0.2270       0.44200   \n",
       "19          0.526          3.400       0.73850        0.8210       0.90350   \n",
       "20          0.106          3.341       0.75925        0.9040       1.17375   \n",
       "21          0.000          3.385       0.17600        0.4490       1.70400   \n",
       "22          0.000          2.551       0.95800        1.0960       1.29300   \n",
       "23          0.053          3.135       0.98200        1.1280       1.36650   \n",
       "24          0.146          3.841       1.75000        1.8560       1.95700   \n",
       "25          0.085          4.276       0.35400        0.4840       0.76950   \n",
       "26          0.086          2.425       1.01200        1.1370       1.23900   \n",
       "27          0.000          1.968       0.10800        0.1480       0.26400   \n",
       "28          0.111          2.365       0.89900        1.0640       1.19850   \n",
       "29          0.000          2.263       0.45600        0.5575       0.98950   \n",
       "30          0.234          2.606       0.75300        0.7930       0.84000   \n",
       "31          0.138          2.154       0.58200        0.6310       0.71100   \n",
       "32          0.067          3.159       0.26400        0.4690       0.72100   \n",
       "33          0.069          2.312       0.28900        0.3960       0.49500   \n",
       "34          0.124          2.368       0.90200        1.0420       1.15100   \n",
       "35          0.122          2.568       0.26200        0.2970       0.34000   \n",
       "36          0.000          2.189       0.12850        0.2080       0.43600   \n",
       "37          0.306          1.100       0.83100        0.8920       0.95200   \n",
       "38          0.085          2.070       0.27450        0.3640       0.45700   \n",
       "\n",
       "    ...  A_P_1_R_T_T_B  A_P_1_S_T_D_E_V_B  A_P_2_R_T_T_B  A_P_2_S_T_D_E_V_B  \\\n",
       "0   ...       7.701247           1.284562       1.045425           0.141963   \n",
       "1   ...       7.319855           1.598138       2.084158           1.720347   \n",
       "2   ...       5.561764           1.070146       2.954241           0.302136   \n",
       "3   ...       5.259823           0.611491       2.998302           0.281244   \n",
       "4   ...       3.841902           0.868242       4.311887           0.598031   \n",
       "5   ...       4.858888           0.526955       7.449432           0.138517   \n",
       "6   ...       2.671067           2.285251       6.636018           1.623611   \n",
       "7   ...       1.179289           0.293636      10.096093           0.168584   \n",
       "8   ...       7.991153           0.173280       2.513745           0.851494   \n",
       "9   ...       6.593670           1.863201       3.477179           0.184102   \n",
       "10  ...       5.991196           0.366653       3.772486           0.123984   \n",
       "11  ...       4.435881           0.702222       5.427796           0.829842   \n",
       "12  ...       4.284173           0.476914       5.046579           1.167715   \n",
       "13  ...       3.696801           0.718538       5.776027           0.579422   \n",
       "14  ...       4.078306           0.768500       6.628178           0.514992   \n",
       "15  ...       4.443587           0.252731       8.893570           0.267026   \n",
       "16  ...       8.135619           1.370246       3.613415           1.179244   \n",
       "17  ...       7.266462           1.195492       3.717670           0.340747   \n",
       "18  ...       7.360543           0.470111       4.049911           0.418374   \n",
       "19  ...       4.966472           0.916813       5.063435           0.889475   \n",
       "20  ...       3.641076           0.825395       8.723374           1.653500   \n",
       "21  ...       9.316530           0.724311       5.758195           1.994493   \n",
       "22  ...       7.631718           1.174695       4.838801           0.798354   \n",
       "23  ...       6.888355           1.417582       5.023658           1.818303   \n",
       "24  ...       7.176985           1.955949       7.687108           0.646799   \n",
       "25  ...       5.289499           0.647979       6.714875           0.200123   \n",
       "26  ...       5.733540           1.174977       6.922496           0.311783   \n",
       "27  ...       6.343611           0.342574       7.792909           0.271413   \n",
       "28  ...       7.609945           1.023094      11.253671           0.292603   \n",
       "29  ...      11.440885           0.705821       5.799641           1.094408   \n",
       "30  ...       9.314909           0.816597       5.544236           0.214925   \n",
       "31  ...       9.274397           0.618753       5.817848           1.605957   \n",
       "32  ...       6.151108           0.737088       7.890436           0.745549   \n",
       "33  ...       8.382122           0.474463       9.475679           0.147662   \n",
       "34  ...      10.039902           1.116149       5.958733           0.144283   \n",
       "35  ...      10.429995           0.424747       7.118130           0.510474   \n",
       "36  ...       9.258583           0.286670       9.062368           0.814729   \n",
       "37  ...       5.726580           0.893827       9.739361           0.799677   \n",
       "38  ...       7.150149           0.495289      11.171871           0.351382   \n",
       "\n",
       "    A_P_3_R_T_T_B  A_P_3_S_T_D_E_V_B  P_r_o_d_u_c_t___R_T_T_B  \\\n",
       "0       10.495666           1.765401                84.501124   \n",
       "1        9.855404           0.802764               150.350479   \n",
       "2        8.922131           0.415342               146.645583   \n",
       "3       11.299793           0.164010               178.183766   \n",
       "4        7.111352           1.792018               117.829467   \n",
       "5        6.057580           0.280483               219.305192   \n",
       "6        7.180249           0.925377               127.267301   \n",
       "7        5.669654           1.064750                67.503082   \n",
       "8       10.413860           0.758885               209.189829   \n",
       "9        9.097593           1.571577               208.578083   \n",
       "10       7.663114           0.271956               173.193674   \n",
       "11       7.626401           0.950328               183.633350   \n",
       "12       6.846530           1.073830               148.005120   \n",
       "13       4.426642           0.739459                94.512869   \n",
       "14       5.811444           0.624411               157.093991   \n",
       "15       4.554344           1.040989               179.950303   \n",
       "16       9.679940           0.805656               284.556338   \n",
       "17       8.455948           0.518209               228.431005   \n",
       "18       7.791006           0.205618               232.239060   \n",
       "19       6.707043           2.137917               168.858753   \n",
       "20       4.555558           0.355508               144.704609   \n",
       "21      11.299842           1.630470               606.713807   \n",
       "22       7.887724           0.548664               291.267644   \n",
       "23       9.576689           0.204503               331.411231   \n",
       "24       7.536028           0.311985               415.805938   \n",
       "25       5.445504           1.113446               193.400901   \n",
       "26       3.556346           0.813111               141.145934   \n",
       "27       3.357062           0.331555               165.945137   \n",
       "28       3.889475           0.646008               333.221078   \n",
       "29       8.492251           0.573785               563.763462   \n",
       "30       9.497439           0.891429               490.283285   \n",
       "31       7.237160           1.708095               390.518487   \n",
       "32       2.730353           0.200416               132.571776   \n",
       "33       1.721144           1.417366               136.691918   \n",
       "34       9.413162           0.136864               563.154420   \n",
       "35       9.894398           0.443328               734.560273   \n",
       "36       7.160629           0.390941               600.908796   \n",
       "37       1.911560           0.142254               106.607213   \n",
       "38       0.476068           0.984149                38.026342   \n",
       "\n",
       "    P_r_o_d_u_c_t___R_T_T_1_2_B  P_r_o_d_u_c_t___R_T_T_2_3  \\\n",
       "0                      8.051205                  10.972355   \n",
       "1                     15.256179                  20.540028   \n",
       "2                     16.434603                  26.360476   \n",
       "3                     15.769445                  33.879835   \n",
       "4                     16.570329                  30.662568   \n",
       "5                     36.191185                  45.124512   \n",
       "6                     17.724517                  47.648455   \n",
       "7                     11.906871                  57.241202   \n",
       "8                     20.087880                  26.178898   \n",
       "9                     22.929268                  31.633872   \n",
       "10                    22.601107                  28.908739   \n",
       "11                    24.078188                  41.394540   \n",
       "12                    21.616111                  34.551484   \n",
       "13                    21.351754                  25.567975   \n",
       "14                    27.031638                  38.519394   \n",
       "15                    39.511292                  40.504470   \n",
       "16                    29.398128                  34.976033   \n",
       "17                    27.013915                  31.436538   \n",
       "18                    29.809048                  31.553261   \n",
       "19                    25.147993                  33.993675   \n",
       "20                    31.761361                  39.739908   \n",
       "21                    53.651169                  65.077842   \n",
       "22                    36.928914                  38.166387   \n",
       "23                    34.605986                  48.109547   \n",
       "24                    55.169271                  57.943407   \n",
       "25                    35.520183                  36.563007   \n",
       "26                    39.688458                  24.618752   \n",
       "27                    49.434571                  26.161287   \n",
       "28                    85.670931                  43.772010   \n",
       "29                    66.369916                  49.265219   \n",
       "30                    51.643667                  52.655095   \n",
       "31                    53.958978                  42.105305   \n",
       "32                    48.555863                  21.544257   \n",
       "33                    79.426609                  16.307616   \n",
       "34                    59.824874                  56.090505   \n",
       "35                    74.240654                  70.428844   \n",
       "36                    83.910441                  64.898712   \n",
       "37                    55.773370                  18.617340   \n",
       "38                    79.875620                   5.319558   \n",
       "\n",
       "    P_r_o_d_u_c_t___R_T_T_1_3_B  \n",
       "0                     80.828673  \n",
       "1                     72.138262  \n",
       "2                     49.623094  \n",
       "3                     59.432941  \n",
       "4                     27.319950  \n",
       "5                     29.443773  \n",
       "6                     19.178980  \n",
       "7                      6.685706  \n",
       "8                     83.214182  \n",
       "9                     59.980020  \n",
       "10                    45.911292  \n",
       "11                    33.830456  \n",
       "12                    29.333792  \n",
       "13                    16.364035  \n",
       "14                    23.700957  \n",
       "15                    20.237850  \n",
       "16                    78.751485  \n",
       "17                    61.445338  \n",
       "18                    57.344499  \n",
       "19                    33.315384  \n",
       "20                    16.588691  \n",
       "21                   105.338581  \n",
       "22                    60.194439  \n",
       "23                    65.968388  \n",
       "24                    54.081431  \n",
       "25                    28.802768  \n",
       "26                    20.390494  \n",
       "27                    21.294621  \n",
       "28                    29.598461  \n",
       "29                    97.156259  \n",
       "30                    88.433425  \n",
       "31                    67.120811  \n",
       "32                    16.793847  \n",
       "33                    14.426766  \n",
       "34                    94.509465  \n",
       "35                   103.198779  \n",
       "36                    66.296987  \n",
       "37                    10.946011  \n",
       "38                     3.403318  \n",
       "\n",
       "[39 rows x 120 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking input data and output data by concatinating two dataframes\n",
    "input_data=grouped.iloc[:,2:] \n",
    "output_data = grouped.iloc[:, :2]\n",
    "first_df=grouped.iloc[:,2:] \n",
    "second_df=groupedd.iloc[:,2:] \n",
    "input_data = pd.concat([first_df, second_df], axis=1)\n",
    "output_data = grouped.iloc[:, :2]\n",
    "input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eaaf6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=np.array(input_data.values)\n",
    "output_data=np.array(output_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13f672d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 2)\n",
      "(39, 120)\n"
     ]
    }
   ],
   "source": [
    "X=input_data\n",
    "y=output_data\n",
    "print(output_data.shape)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b57485e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used standardscaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.fit_transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d5e2edd",
   "metadata": {},
   "source": [
    "**2. Test with Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df86137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 4, 'n_estimators': 100}\n",
      "Mean Squared Error in meter: 0.863\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.929\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 28.226\n",
      "R2 score is in percent: 79.70\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,RF_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, RF_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "684b2947",
   "metadata": {},
   "source": [
    "**3. Testing with KNN Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "592f6eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 1.29\n",
      "Root Mean Squared Error (RMSE) on new data in m: 1.13\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 37.806\n",
      "R2 score is in percent: 67.16\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from math import sqrt\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,knn_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, knn_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae2b77b3",
   "metadata": {},
   "source": [
    "**4. Testing with DNN Regressor Model and Hyperparameter Tuning by using Keras Tuner**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b43e67bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras_tuner import RandomSearch\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "# Define the model architecture\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dense(units=hp.Int('units_1', min_value=32, max_value=512, step=16), input_shape=(120,), activation='relu'))\n",
    "    model.add(keras.layers.Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "    for i in range(hp.Int('num_hidden_layers', 1, 10)):\n",
    "        model.add(keras.layers.Dense(units=hp.Int('units_' + str(i+2), min_value=32, max_value=512, step=16),\n",
    "                                 activation=hp.Choice('activation_' + str(i+2), values=['relu', 'sigmoid', 'tanh'])))\n",
    "        model.add(keras.layers.Dropout(hp.Float('dropout_' + str(i+2), min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    \n",
    "        \n",
    "    model.add(keras.layers.Dense(units=2, activation='linear'))\n",
    "    \n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam(\n",
    "                        hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
    "\n",
    "                  loss='mean_squared_error',\n",
    "                  metrics=['mse'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb6d60b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from testtt_new_copy\\hellooo_new_copy\\tuner0.json\n"
     ]
    }
   ],
   "source": [
    "# Define the search space\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    executions_per_trial=3,\n",
    "    directory='testtt_new_copy',\n",
    "    project_name='hellooo_new_copy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b34d40d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train, y_train,\n",
    "             epochs=200,\n",
    "           validation_data=(X_test,y_test) ,\n",
    "             callbacks=[keras.callbacks.EarlyStopping(patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64811e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in testtt_new_copy\\hellooo_new_copy\n",
      "Showing 10 best trials\n",
      "<keras_tuner.engine.objective.Objective object at 0x0000029846D9D8E0>\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 400\n",
      "dropout_1: 0.0\n",
      "num_hidden_layers: 1\n",
      "units_2: 112\n",
      "activation_2: sigmoid\n",
      "dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "units_3: 192\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.1\n",
      "units_4: 336\n",
      "activation_4: relu\n",
      "dropout_4: 0.4\n",
      "units_5: 432\n",
      "activation_5: relu\n",
      "dropout_5: 0.0\n",
      "units_6: 512\n",
      "activation_6: tanh\n",
      "dropout_6: 0.1\n",
      "units_7: 432\n",
      "activation_7: relu\n",
      "dropout_7: 0.30000000000000004\n",
      "units_8: 96\n",
      "activation_8: tanh\n",
      "dropout_8: 0.2\n",
      "units_9: 432\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.4\n",
      "units_10: 256\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.4\n",
      "units_11: 272\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.30000000000000004\n",
      "Score: 0.604464719692866\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 288\n",
      "dropout_1: 0.4\n",
      "num_hidden_layers: 3\n",
      "units_2: 144\n",
      "activation_2: relu\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.0001\n",
      "units_3: 256\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.1\n",
      "units_4: 176\n",
      "activation_4: relu\n",
      "dropout_4: 0.0\n",
      "units_5: 112\n",
      "activation_5: tanh\n",
      "dropout_5: 0.1\n",
      "units_6: 480\n",
      "activation_6: tanh\n",
      "dropout_6: 0.0\n",
      "units_7: 512\n",
      "activation_7: tanh\n",
      "dropout_7: 0.1\n",
      "units_8: 48\n",
      "activation_8: relu\n",
      "dropout_8: 0.1\n",
      "units_9: 128\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.30000000000000004\n",
      "units_10: 368\n",
      "activation_10: relu\n",
      "dropout_10: 0.1\n",
      "units_11: 352\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.2\n",
      "Score: 0.8210774461428324\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 400\n",
      "dropout_1: 0.0\n",
      "num_hidden_layers: 1\n",
      "units_2: 80\n",
      "activation_2: tanh\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.01\n",
      "units_3: 144\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.0\n",
      "units_4: 128\n",
      "activation_4: relu\n",
      "dropout_4: 0.4\n",
      "units_5: 464\n",
      "activation_5: relu\n",
      "dropout_5: 0.2\n",
      "units_6: 496\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.0\n",
      "units_7: 432\n",
      "activation_7: relu\n",
      "dropout_7: 0.4\n",
      "units_8: 240\n",
      "activation_8: sigmoid\n",
      "dropout_8: 0.30000000000000004\n",
      "units_9: 64\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.30000000000000004\n",
      "units_10: 256\n",
      "activation_10: relu\n",
      "dropout_10: 0.1\n",
      "units_11: 352\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.4\n",
      "Score: 1.005285422007243\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 336\n",
      "dropout_1: 0.2\n",
      "num_hidden_layers: 2\n",
      "units_2: 256\n",
      "activation_2: relu\n",
      "dropout_2: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "units_3: 32\n",
      "activation_3: relu\n",
      "dropout_3: 0.0\n",
      "Score: 1.2655504941940308\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 288\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 7\n",
      "units_2: 80\n",
      "activation_2: relu\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.001\n",
      "units_3: 112\n",
      "activation_3: tanh\n",
      "dropout_3: 0.4\n",
      "units_4: 32\n",
      "activation_4: relu\n",
      "dropout_4: 0.0\n",
      "units_5: 32\n",
      "activation_5: relu\n",
      "dropout_5: 0.0\n",
      "units_6: 32\n",
      "activation_6: relu\n",
      "dropout_6: 0.0\n",
      "units_7: 32\n",
      "activation_7: relu\n",
      "dropout_7: 0.0\n",
      "units_8: 32\n",
      "activation_8: relu\n",
      "dropout_8: 0.0\n",
      "Score: 1.2967978517214458\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 448\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 3\n",
      "units_2: 32\n",
      "activation_2: relu\n",
      "dropout_2: 0.4\n",
      "learning_rate: 0.0001\n",
      "units_3: 448\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.0\n",
      "units_4: 256\n",
      "activation_4: sigmoid\n",
      "dropout_4: 0.0\n",
      "units_5: 368\n",
      "activation_5: relu\n",
      "dropout_5: 0.4\n",
      "units_6: 32\n",
      "activation_6: tanh\n",
      "dropout_6: 0.2\n",
      "units_7: 384\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.4\n",
      "units_8: 496\n",
      "activation_8: tanh\n",
      "dropout_8: 0.0\n",
      "units_9: 80\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.0\n",
      "units_10: 96\n",
      "activation_10: tanh\n",
      "dropout_10: 0.30000000000000004\n",
      "units_11: 176\n",
      "activation_11: relu\n",
      "dropout_11: 0.30000000000000004\n",
      "Score: 1.4233957529067993\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 464\n",
      "dropout_1: 0.0\n",
      "num_hidden_layers: 2\n",
      "units_2: 256\n",
      "activation_2: sigmoid\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.001\n",
      "units_3: 480\n",
      "activation_3: sigmoid\n",
      "dropout_3: 0.30000000000000004\n",
      "units_4: 160\n",
      "activation_4: sigmoid\n",
      "dropout_4: 0.1\n",
      "units_5: 128\n",
      "activation_5: relu\n",
      "dropout_5: 0.0\n",
      "units_6: 96\n",
      "activation_6: tanh\n",
      "dropout_6: 0.2\n",
      "units_7: 176\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.2\n",
      "units_8: 304\n",
      "activation_8: sigmoid\n",
      "dropout_8: 0.30000000000000004\n",
      "units_9: 128\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.4\n",
      "units_10: 352\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.0\n",
      "units_11: 80\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.4\n",
      "Score: 1.5874963998794556\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 224\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 6\n",
      "units_2: 112\n",
      "activation_2: relu\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.01\n",
      "units_3: 400\n",
      "activation_3: relu\n",
      "dropout_3: 0.30000000000000004\n",
      "units_4: 288\n",
      "activation_4: tanh\n",
      "dropout_4: 0.0\n",
      "units_5: 160\n",
      "activation_5: tanh\n",
      "dropout_5: 0.4\n",
      "units_6: 368\n",
      "activation_6: relu\n",
      "dropout_6: 0.30000000000000004\n",
      "units_7: 352\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.30000000000000004\n",
      "units_8: 240\n",
      "activation_8: relu\n",
      "dropout_8: 0.0\n",
      "units_9: 448\n",
      "activation_9: relu\n",
      "dropout_9: 0.2\n",
      "units_10: 128\n",
      "activation_10: sigmoid\n",
      "dropout_10: 0.30000000000000004\n",
      "units_11: 176\n",
      "activation_11: relu\n",
      "dropout_11: 0.2\n",
      "Score: 1.7424612045288086\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 224\n",
      "dropout_1: 0.30000000000000004\n",
      "num_hidden_layers: 4\n",
      "units_2: 288\n",
      "activation_2: tanh\n",
      "dropout_2: 0.1\n",
      "learning_rate: 0.001\n",
      "units_3: 320\n",
      "activation_3: relu\n",
      "dropout_3: 0.30000000000000004\n",
      "units_4: 464\n",
      "activation_4: tanh\n",
      "dropout_4: 0.0\n",
      "units_5: 32\n",
      "activation_5: sigmoid\n",
      "dropout_5: 0.0\n",
      "units_6: 480\n",
      "activation_6: relu\n",
      "dropout_6: 0.4\n",
      "units_7: 224\n",
      "activation_7: relu\n",
      "dropout_7: 0.4\n",
      "units_8: 368\n",
      "activation_8: tanh\n",
      "dropout_8: 0.0\n",
      "units_9: 224\n",
      "activation_9: relu\n",
      "dropout_9: 0.0\n",
      "units_10: 112\n",
      "activation_10: tanh\n",
      "dropout_10: 0.30000000000000004\n",
      "units_11: 96\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.2\n",
      "Score: 2.503242611885071\n",
      "Trial summary\n",
      "Hyperparameters:\n",
      "units_1: 448\n",
      "dropout_1: 0.4\n",
      "num_hidden_layers: 7\n",
      "units_2: 320\n",
      "activation_2: tanh\n",
      "dropout_2: 0.0\n",
      "learning_rate: 0.0001\n",
      "units_3: 304\n",
      "activation_3: tanh\n",
      "dropout_3: 0.30000000000000004\n",
      "units_4: 336\n",
      "activation_4: relu\n",
      "dropout_4: 0.0\n",
      "units_5: 496\n",
      "activation_5: tanh\n",
      "dropout_5: 0.2\n",
      "units_6: 288\n",
      "activation_6: sigmoid\n",
      "dropout_6: 0.4\n",
      "units_7: 96\n",
      "activation_7: sigmoid\n",
      "dropout_7: 0.2\n",
      "units_8: 480\n",
      "activation_8: sigmoid\n",
      "dropout_8: 0.1\n",
      "units_9: 512\n",
      "activation_9: sigmoid\n",
      "dropout_9: 0.0\n",
      "units_10: 240\n",
      "activation_10: relu\n",
      "dropout_10: 0.0\n",
      "units_11: 288\n",
      "activation_11: sigmoid\n",
      "dropout_11: 0.1\n",
      "Score: 2.7332235972086587\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42e91bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 723ms/step - loss: 18.9977 - mse: 18.9977 - val_loss: 19.8354 - val_mse: 19.8354\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 15.8773 - mse: 15.8773 - val_loss: 16.9478 - val_mse: 16.9478\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.7277 - mse: 13.7277 - val_loss: 14.3507 - val_mse: 14.3507\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.4768 - mse: 11.4768 - val_loss: 12.0314 - val_mse: 12.0314\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.2572 - mse: 9.2572 - val_loss: 9.9933 - val_mse: 9.9933\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.3113 - mse: 8.3113 - val_loss: 8.2194 - val_mse: 8.2194\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.7843 - mse: 5.7843 - val_loss: 6.7089 - val_mse: 6.7089\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.6545 - mse: 4.6545 - val_loss: 5.4563 - val_mse: 5.4563\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.2819 - mse: 4.2819 - val_loss: 4.4369 - val_mse: 4.4369\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.3337 - mse: 3.3337 - val_loss: 3.6258 - val_mse: 3.6258\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4211 - mse: 2.4211 - val_loss: 2.9977 - val_mse: 2.9977\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0764 - mse: 3.0764 - val_loss: 2.5117 - val_mse: 2.5117\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.8979 - mse: 1.8979 - val_loss: 2.1465 - val_mse: 2.1465\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7347 - mse: 1.7347 - val_loss: 1.8726 - val_mse: 1.8726\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.6844 - mse: 1.6844 - val_loss: 1.6668 - val_mse: 1.6668\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9938 - mse: 1.9938 - val_loss: 1.5075 - val_mse: 1.5075\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5510 - mse: 1.5510 - val_loss: 1.3835 - val_mse: 1.3835\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9320 - mse: 1.9320 - val_loss: 1.2811 - val_mse: 1.2811\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4368 - mse: 1.4368 - val_loss: 1.1938 - val_mse: 1.1938\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4594 - mse: 1.4594 - val_loss: 1.1193 - val_mse: 1.1193\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3031 - mse: 1.3031 - val_loss: 1.0535 - val_mse: 1.0535\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1342 - mse: 1.1342 - val_loss: 0.9988 - val_mse: 0.9988\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4224 - mse: 1.4224 - val_loss: 0.9528 - val_mse: 0.9528\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1485 - mse: 1.1485 - val_loss: 0.9170 - val_mse: 0.9170\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1598 - mse: 1.1598 - val_loss: 0.8894 - val_mse: 0.8894\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9428 - mse: 0.9428 - val_loss: 0.8691 - val_mse: 0.8691\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1212 - mse: 1.1212 - val_loss: 0.8544 - val_mse: 0.8544\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0301 - mse: 1.0301 - val_loss: 0.8455 - val_mse: 0.8455\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1202 - mse: 1.1202 - val_loss: 0.8403 - val_mse: 0.8403\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9511 - mse: 0.9511 - val_loss: 0.8388 - val_mse: 0.8388\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2759 - mse: 1.2759 - val_loss: 0.8324 - val_mse: 0.8324\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1381 - mse: 1.1381 - val_loss: 0.8265 - val_mse: 0.8265\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0470 - mse: 1.0470 - val_loss: 0.8208 - val_mse: 0.8208\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7141 - mse: 0.7141 - val_loss: 0.8130 - val_mse: 0.8130\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1327 - mse: 1.1327 - val_loss: 0.8051 - val_mse: 0.8051\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6893 - mse: 0.6893 - val_loss: 0.7916 - val_mse: 0.7916\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9594 - mse: 0.9594 - val_loss: 0.7759 - val_mse: 0.7759\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8951 - mse: 0.8951 - val_loss: 0.7590 - val_mse: 0.7590\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7534 - mse: 0.7534 - val_loss: 0.7445 - val_mse: 0.7445\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6047 - mse: 0.6047 - val_loss: 0.7293 - val_mse: 0.7293\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.8058 - mse: 0.8058 - val_loss: 0.7105 - val_mse: 0.7105\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8403 - mse: 0.8403 - val_loss: 0.6984 - val_mse: 0.6984\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8474 - mse: 0.8474 - val_loss: 0.6800 - val_mse: 0.6800\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7186 - mse: 0.7186 - val_loss: 0.6618 - val_mse: 0.6618\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7097 - mse: 0.7097 - val_loss: 0.6479 - val_mse: 0.6479\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4662 - mse: 0.4662 - val_loss: 0.6336 - val_mse: 0.6336\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6621 - mse: 0.6621 - val_loss: 0.6221 - val_mse: 0.6221\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6552 - mse: 0.6552 - val_loss: 0.6108 - val_mse: 0.6108\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6587 - mse: 0.6587 - val_loss: 0.5995 - val_mse: 0.5995\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6519 - mse: 0.6519 - val_loss: 0.5894 - val_mse: 0.5894\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6362 - mse: 0.6362 - val_loss: 0.5826 - val_mse: 0.5826\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6630 - mse: 0.6630 - val_loss: 0.5738 - val_mse: 0.5738\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6675 - mse: 0.6675 - val_loss: 0.5646 - val_mse: 0.5646\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5835 - mse: 0.5835 - val_loss: 0.5572 - val_mse: 0.5572\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5608 - mse: 0.5608 - val_loss: 0.5519 - val_mse: 0.5519\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0624 - mse: 1.0624 - val_loss: 0.5516 - val_mse: 0.5516\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6873 - mse: 0.6873 - val_loss: 0.5559 - val_mse: 0.5559\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0111 - mse: 1.0111 - val_loss: 0.5578 - val_mse: 0.5578\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6104 - mse: 0.6104 - val_loss: 0.5599 - val_mse: 0.5599\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5880 - mse: 0.5880 - val_loss: 0.5664 - val_mse: 0.5664\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5932 - mse: 0.5932 - val_loss: 0.5760 - val_mse: 0.5760\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4345 - mse: 0.4345 - val_loss: 0.5844 - val_mse: 0.5844\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5470 - mse: 0.5470 - val_loss: 0.5910 - val_mse: 0.5910\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5144 - mse: 0.5144 - val_loss: 0.5973 - val_mse: 0.5973\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6646 - mse: 0.6646 - val_loss: 0.6007 - val_mse: 0.6007\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5432 - mse: 0.5432 - val_loss: 0.6026 - val_mse: 0.6026\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5380 - mse: 0.5380 - val_loss: 0.6018 - val_mse: 0.6018\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5920 - mse: 0.5920 - val_loss: 0.5931 - val_mse: 0.5931\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4233 - mse: 0.4233 - val_loss: 0.5872 - val_mse: 0.5872\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4263 - mse: 0.4263 - val_loss: 0.5813 - val_mse: 0.5813\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3904 - mse: 0.3904 - val_loss: 0.5767 - val_mse: 0.5767\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5777 - mse: 0.5777 - val_loss: 0.5706 - val_mse: 0.5706\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3618 - mse: 0.3618 - val_loss: 0.5627 - val_mse: 0.5627\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4571 - mse: 0.4571 - val_loss: 0.5532 - val_mse: 0.5532\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4934 - mse: 0.4934 - val_loss: 0.5446 - val_mse: 0.5446\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4294 - mse: 0.4294 - val_loss: 0.5361 - val_mse: 0.5361\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5031 - mse: 0.5031 - val_loss: 0.5272 - val_mse: 0.5272\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5138 - mse: 0.5138 - val_loss: 0.5179 - val_mse: 0.5179\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5699 - mse: 0.5699 - val_loss: 0.5085 - val_mse: 0.5085\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4580 - mse: 0.4580 - val_loss: 0.5011 - val_mse: 0.5011\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3468 - mse: 0.3468 - val_loss: 0.4974 - val_mse: 0.4974\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4016 - mse: 0.4016 - val_loss: 0.4981 - val_mse: 0.4981\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6808 - mse: 0.6808 - val_loss: 0.4999 - val_mse: 0.4999\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5710 - mse: 0.5710 - val_loss: 0.5009 - val_mse: 0.5009\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3646 - mse: 0.3646 - val_loss: 0.5041 - val_mse: 0.5041\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4994 - mse: 0.4994 - val_loss: 0.5084 - val_mse: 0.5084\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6758 - mse: 0.6758 - val_loss: 0.5113 - val_mse: 0.5113\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4200 - mse: 0.4200 - val_loss: 0.5147 - val_mse: 0.5147\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4749 - mse: 0.4749 - val_loss: 0.5198 - val_mse: 0.5198\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4859 - mse: 0.4859 - val_loss: 0.5226 - val_mse: 0.5226\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4214 - mse: 0.4214 - val_loss: 0.5225 - val_mse: 0.5225\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5772 - mse: 0.5772 - val_loss: 0.5248 - val_mse: 0.5248\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5127 - mse: 0.5127 - val_loss: 0.5346 - val_mse: 0.5346\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5212 - mse: 0.5212 - val_loss: 0.5425 - val_mse: 0.5425\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5571 - mse: 0.5571 - val_loss: 0.5457 - val_mse: 0.5457\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4760 - mse: 0.4760 - val_loss: 0.5440 - val_mse: 0.5440\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5497 - mse: 0.5497 - val_loss: 0.5382 - val_mse: 0.5382\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5308 - mse: 0.5308 - val_loss: 0.5343 - val_mse: 0.5343\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3744 - mse: 0.3744 - val_loss: 0.5255 - val_mse: 0.5255\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4175 - mse: 0.4175 - val_loss: 0.5134 - val_mse: 0.5134\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3829 - mse: 0.3829 - val_loss: 0.5046 - val_mse: 0.5046\n"
     ]
    }
   ],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "dnn_model = tuner.hypermodel.build(best_hps)\n",
    "#best_model = tuner.get_best_models()[0]\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "n_epochs=500\n",
    "#history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=32, validation_split=0.2,callbacks=callbacks_list)\n",
    "#history = model.fit(X_train, y_train, epochs=n_epochs, bbatch_size=32, validation_split=0.2,callbacks=callbacks_list)\n",
    "history = dnn_model.fit(X_train, y_train, epochs=n_epochs, batch_size=32, validation_data=(X_test,y_test),callbacks=[keras.callbacks.EarlyStopping(patience=20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a30359e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5046 - mse: 0.5046\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_mse = dnn_model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11ce2fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('DNN Model loss Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42d26cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.087154  , 6.97905   ],\n",
       "       [5.4793777 , 3.26837   ],\n",
       "       [1.3031187 , 3.6499555 ],\n",
       "       [2.7883952 , 6.1829753 ],\n",
       "       [4.683746  , 0.67453885],\n",
       "       [4.257589  , 6.365557  ],\n",
       "       [1.7824749 , 6.5207953 ],\n",
       "       [4.4335003 , 6.842014  ],\n",
       "       [4.240049  , 2.6601367 ],\n",
       "       [2.9219003 , 6.904313  ],\n",
       "       [2.4153347 , 1.5384728 ],\n",
       "       [2.7035422 , 1.2881352 ]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=dnn_model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9b9bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on new data in m: 0.50\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.71\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 21.71\n",
      "R2 score is in percent: 87.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, y_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(y_test,y_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, y_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0a41234",
   "metadata": {},
   "source": [
    "**5. Random Regressor for DNN output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a194bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 4, 'n_estimators': 200}\n",
      "Mean Squared Error in meter: 0.226\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.475\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 14.433\n",
      "R2 score is in percent: 93.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(y_pred, y_test)\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(y_pred)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.3f}'.format(mean_absolute_percentage_error(y_test,RF_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, RF_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b759450",
   "metadata": {},
   "source": [
    "**6. KNN Regressor for DNN Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ba1ffec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 0.81\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.90\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 26.88\n",
      "R2 score is in percent: 78.95\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(y_pred, y_test)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(y_pred)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(y_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "rmse=sqrt(mean_squared_error(y_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(y_test,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(y_test, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2572f0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for saving DNN Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "dnn_model.save('m3y_DNN_Regrr_model_xy_test_copy.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "201927f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2985285f4c0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = load_model('m3y_DNN_Regrr_model_xy_test_copy.h5')\n",
    "loaded_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b117e91",
   "metadata": {},
   "source": [
    "**7. Validate the model with Unknown/online dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea9f116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dfff=pd.read_csv('D:/testt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c11b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfff.drop(['AP1RSS','AP2RSS','AP3RSS'],axis=1,inplace=True)\n",
    "#dffff=dfff.assign(Product_RTT=dfff['AP1RTT']*dfff['AP2RTT']*dfff['AP3RTT'],Product_RTT12=dfff['AP1RTT']*dfff['AP2RTT'],Product_RTT23=dfff['AP2RTT']*dfff['AP3RTT'],Product_RTT13=dfff['AP1RTT']*dfff['AP3RTT'],square_RTT1=dfff['AP1RTT']*dfff['AP1RTT'],square_RTT2=dfff['AP2RTT']*dfff['AP2RTT'],square_RTT3=dfff['AP3RTT']*dfff['AP3RTT'])\n",
    "dffff=dfff.assign(Product_RTT=dfff['AP1RTT']*dfff['AP2RTT']*dfff['AP3RTT'],Product_RTT12=dfff['AP1RTT']*dfff['AP2RTT'],Product_RTT23=dfff['AP2RTT']*dfff['AP3RTT'],Product_RTT13=dfff['AP1RTT']*dfff['AP3RTT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f2a78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the DataFrame by 'x' and 'y'\n",
    "groups = dffff.groupby(['x', 'y'])\n",
    "\n",
    "# Split the groups into two separate dataframes\n",
    "df1 = pd.concat([group.iloc[:len(group) // 2] for _, group in groups])\n",
    "df2 = pd.concat([group.iloc[len(group) // 2:] for _, group in groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ba7505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTT':'AP1RTTA','AP1STDEV':'AP1STDEVA','AP2RTT':'AP2RTTA','AP2STDEV':'AP2STDEVA','AP3RTT':'AP3RTTA','AP3STDEV':'AP3STDEVA','Product_RTT':'Product_RTTA','Product_RTT12':'Product_RTT12A','Product_RTT13':'Product_RTT13A','square_RTT1':'square_RTT1A','square_RTT2':'square_RTT2A','square_RTT3':'square_RTT3A'}\n",
    "df1.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07fa77c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTT':'AP1RTTB','AP1STDEV':'AP1STDEVB','AP2RTT':'AP2RTTB','AP2STDEV':'AP2STDEVB','AP3RTT':'AP3RTTB','AP3STDEV':'AP3STDEVB','Product_RTT':'Product_RTTB','Product_RTT12':'Product_RTT12B','Product_RTT13':'Product_RTT13B','square_RTT1':'square_RTT1B','square_RTT2':'square_RTT2B','square_RTT3':'square_RTT3B'}\n",
    "df2.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c308e9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the data by x and y values, and calculate the mean of each group\n",
    "groupedd1 = df1.groupby(['x', 'y']).mean()\n",
    "groupedd2 = df2.groupby(['x', 'y']).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "120fdf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'x' and 'y', and calculate the minimum, maximum, 25th, 50th, and 75th percentiles for each column for local feature extractions\n",
    "grouped = df1.groupby(['x', 'y']).agg(['min', 'max', lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=50), lambda x: np.percentile(x, q=75)])\n",
    "\n",
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped_mean = df1.groupby(['x', 'y']).mean()\n",
    "grouped = pd.concat([grouped, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "grouped.columns = ['_'.join(col).strip() for col in grouped.columns.values]\n",
    "grouped = grouped.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b1dd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by 'x' and 'y', and calculate the minimum, maximum, 25th, 50th, and 75th percentiles for each column for local feature extractions\n",
    "groupedd = df2.groupby(['x', 'y']).agg(['min', 'max', lambda x: np.percentile(x, q=25), lambda x: np.percentile(x, q=50), lambda x: np.percentile(x, q=75)])\n",
    "\n",
    "# Add the mean or average value of each column to the grouped dataframe\n",
    "grouped_mean = df2.groupby(['x', 'y']).mean()\n",
    "groupedd = pd.concat([groupedd, grouped_mean], axis=1)\n",
    "\n",
    "# Rename the columns and reset the index\n",
    "groupedd.columns = ['_'.join(col).strip() for col in groupedd.columns.values]\n",
    "groupedd = groupedd.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbbf1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTTA_<lambda_0>': 'AP1RTTA_25','AP2RTTA_<lambda_0>': 'AP2RTTA_25','AP3RTTA_<lambda_0>': 'AP3RTTA_25',\n",
    "          'AP1RTTA_<lambda_1>': 'AP1RTTA_50','AP2RTTA_<lambda_1>': 'AP2RTTA_50','AP3RTTA_<lambda_1>': 'AP3RTTA_50',\n",
    "          'AP1RTTA_<lambda_2>': 'AP1RTTA_75','AP2RTTA_<lambda_2>': 'AP2RTTA_75','AP3RTTA_<lambda_2>': 'AP3RTTA_75',\n",
    "           \n",
    "             \n",
    "          'AP1STDEVA_<lambda_0>':'AP1STDEVA_25','AP2STDEVA_<lambda_0>':'AP2STDEVA_25','AP3STDEVA_<lambda_0>':'AP3STDEVA_25',\n",
    "          'AP1STDEVA_<lambda_1>':'AP1STDEVA_50','AP2STDEVA_<lambda_1>':'AP2STDEVA_50','AP3STDEVA_<lambda_1>':'AP3STDEVA_50',\n",
    "          'AP1STDEVA_<lambda_2>':'AP1STDEVA_75','AP2STDEVA_<lambda_2>':'AP2STDEVA_75','AP3STDEVA_<lambda_2>':'AP3STDEVA_75',\n",
    "          'A_P_1_R_T_T_A':'AP1RTTA_MEAN','A_P_2_R_T_T_A':'AP2RTTA_MEAN','A_P_3_R_T_T_A_A':'AP3RTTA_MEAN',\n",
    "          'A_P_1_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_2_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_3_S_T_D_E_V_A':'AP1STDEVA_MEAN'}\n",
    "grouped.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5fb1ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_name={'AP1RTTB_<lambda_0>': 'AP1RTTB_25','AP2RTTB_<lambda_0>': 'AP2RTTB_25','AP3RTTB_<lambda_0>': 'AP3RTTB_25',\n",
    "          'AP1RTTB_<lambda_1>': 'AP1RTTB_50','AP2RTTB_<lambda_1>': 'AP2RTTB_50','AP3RTTB_<lambda_1>': 'AP3RTTB_50',\n",
    "          'AP1RTTB_<lambda_2>': 'AP1RTTB_75','AP2RTTB_<lambda_2>': 'AP2RTTB_75','AP3RTTB_<lambda_2>': 'AP3RTTB_75',\n",
    "           \n",
    "             \n",
    "          'AP1STDEVB_<lambda_0>':'AP1STDEVB_25','AP2STDEVB_<lambda_0>':'AP2STDEVB_25','AP3STDEVB_<lambda_0>':'AP3STDEVB_25',\n",
    "          'AP1STDEVB_<lambda_1>':'AP1STDEVB_50','AP2STDEVB_<lambda_1>':'AP2STDEVB_50','AP3STDEVB_<lambda_1>':'AP3STDEVB_50',\n",
    "          'AP1STDEVB_<lambda_2>':'AP1STDEVB_75','AP2STDEVB_<lambda_2>':'AP2STDEVB_75','AP3STDEVB_<lambda_2>':'AP3STDEVB_75',\n",
    "          'A_P_1_R_T_T_A':'AP1RTTA_MEAN','A_P_2_R_T_T_A':'AP2RTTA_MEAN','A_P_3_R_T_T_A_A':'AP3RTTA_MEAN',\n",
    "          'A_P_1_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_2_S_T_D_E_V_A':'AP1STDEVA_MEAN','A_P_3_S_T_D_E_V_A':'AP1STDEVA_MEAN'}\n",
    "groupedd.rename(columns=new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cffd87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=grouped.iloc[:,2:] \n",
    "output_data = grouped.iloc[:, :2]\n",
    "first_df=grouped.iloc[:,2:] \n",
    "second_df=groupedd.iloc[:,2:] \n",
    "input_data = pd.concat([first_df, second_df], axis=1)\n",
    "output_data = grouped.iloc[:, :2]\n",
    "input_data=np.array(input_data.values)\n",
    "output_data=np.array(output_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "240dfa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 2)\n",
      "(39, 120)\n"
     ]
    }
   ],
   "source": [
    "XX=input_data\n",
    "yy=output_data\n",
    "print(output_data.shape)\n",
    "print(input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bef72809",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "XX= sc.fit_transform(XX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3fe7d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.3637596 , 1.3591738 ],\n",
       "       [1.5854747 , 2.2112765 ],\n",
       "       [1.3427453 , 3.0841668 ],\n",
       "       [0.86467385, 3.2519305 ],\n",
       "       [1.224469  , 4.0340943 ],\n",
       "       [1.4003772 , 6.0395722 ],\n",
       "       [2.29464   , 6.681105  ],\n",
       "       [1.4156963 , 7.0128007 ],\n",
       "       [2.570125  , 1.4968024 ],\n",
       "       [2.0773516 , 1.656634  ],\n",
       "       [1.5354359 , 3.3019524 ],\n",
       "       [1.0881388 , 3.5899806 ],\n",
       "       [2.091867  , 4.637921  ],\n",
       "       [2.8422472 , 6.428543  ],\n",
       "       [2.1143453 , 6.1829085 ],\n",
       "       [2.6899564 , 7.044849  ],\n",
       "       [2.398405  , 0.9106165 ],\n",
       "       [2.303809  , 1.5122579 ],\n",
       "       [2.5115733 , 2.3274999 ],\n",
       "       [3.0229857 , 3.7395072 ],\n",
       "       [2.6598287 , 7.314532  ],\n",
       "       [4.071383  , 1.2310385 ],\n",
       "       [2.64289   , 1.4969817 ],\n",
       "       [3.773801  , 2.4770281 ],\n",
       "       [4.2192645 , 4.0216484 ],\n",
       "       [2.6831317 , 5.0897527 ],\n",
       "       [4.3451424 , 6.657986  ],\n",
       "       [4.593656  , 6.932121  ],\n",
       "       [4.5049577 , 7.370806  ],\n",
       "       [5.052073  , 1.2687024 ],\n",
       "       [4.6362014 , 0.7934836 ],\n",
       "       [4.2898107 , 2.0658026 ],\n",
       "       [4.826958  , 7.0390334 ],\n",
       "       [5.048162  , 7.044384  ],\n",
       "       [5.2347674 , 1.0161984 ],\n",
       "       [5.5656695 , 1.4586704 ],\n",
       "       [5.5990276 , 4.0488105 ],\n",
       "       [4.9786363 , 7.1555924 ],\n",
       "       [5.0624003 , 7.1419415 ]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predd=loaded_model.predict(XX)\n",
    "y_predd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "62e0aca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on new data in mm: 0.36\n",
      "Root Mean Squared Error (RMSE) on new data in mm: 0.60\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 18.39\n",
      "R2 score is in percent: 90.68\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(yy, y_predd)\n",
    "print('Mean Squared Error (MSE) on new data in mm: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy, y_predd)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in mm: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,y_predd)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, y_predd)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c1f0360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.363760</td>\n",
       "      <td>1.359174</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.585475</td>\n",
       "      <td>2.211277</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.342745</td>\n",
       "      <td>3.084167</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.864674</td>\n",
       "      <td>3.251930</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.224469</td>\n",
       "      <td>4.034094</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.400377</td>\n",
       "      <td>6.039572</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.294640</td>\n",
       "      <td>6.681105</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.415696</td>\n",
       "      <td>7.012801</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.570125</td>\n",
       "      <td>1.496802</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.077352</td>\n",
       "      <td>1.656634</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.535436</td>\n",
       "      <td>3.301952</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.088139</td>\n",
       "      <td>3.589981</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.091867</td>\n",
       "      <td>4.637921</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.842247</td>\n",
       "      <td>6.428543</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.114345</td>\n",
       "      <td>6.182909</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.689956</td>\n",
       "      <td>7.044849</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.398405</td>\n",
       "      <td>0.910617</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.303809</td>\n",
       "      <td>1.512258</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.511573</td>\n",
       "      <td>2.327500</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.022986</td>\n",
       "      <td>3.739507</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.659829</td>\n",
       "      <td>7.314532</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.071383</td>\n",
       "      <td>1.231038</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.642890</td>\n",
       "      <td>1.496982</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.773801</td>\n",
       "      <td>2.477028</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.219265</td>\n",
       "      <td>4.021648</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.683132</td>\n",
       "      <td>5.089753</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.345142</td>\n",
       "      <td>6.657986</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.593656</td>\n",
       "      <td>6.932121</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.504958</td>\n",
       "      <td>7.370806</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.052073</td>\n",
       "      <td>1.268702</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.636201</td>\n",
       "      <td>0.793484</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.289811</td>\n",
       "      <td>2.065803</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4.826958</td>\n",
       "      <td>7.039033</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.048162</td>\n",
       "      <td>7.044384</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.234767</td>\n",
       "      <td>1.016198</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.565670</td>\n",
       "      <td>1.458670</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.599028</td>\n",
       "      <td>4.048810</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.978636</td>\n",
       "      <td>7.155592</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.062400</td>\n",
       "      <td>7.141942</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_x  predicted_y  x  y\n",
       "0      1.363760     1.359174  1  1\n",
       "1      1.585475     2.211277  1  2\n",
       "2      1.342745     3.084167  1  3\n",
       "3      0.864674     3.251930  1  4\n",
       "4      1.224469     4.034094  1  5\n",
       "5      1.400377     6.039572  1  6\n",
       "6      2.294640     6.681105  1  7\n",
       "7      1.415696     7.012801  1  8\n",
       "8      2.570125     1.496802  2  1\n",
       "9      2.077352     1.656634  2  2\n",
       "10     1.535436     3.301952  2  3\n",
       "11     1.088139     3.589981  2  4\n",
       "12     2.091867     4.637921  2  5\n",
       "13     2.842247     6.428543  2  6\n",
       "14     2.114345     6.182909  2  7\n",
       "15     2.689956     7.044849  2  8\n",
       "16     2.398405     0.910617  3  1\n",
       "17     2.303809     1.512258  3  2\n",
       "18     2.511573     2.327500  3  3\n",
       "19     3.022986     3.739507  3  4\n",
       "20     2.659829     7.314532  3  8\n",
       "21     4.071383     1.231038  4  1\n",
       "22     2.642890     1.496982  4  2\n",
       "23     3.773801     2.477028  4  3\n",
       "24     4.219265     4.021648  4  4\n",
       "25     2.683132     5.089753  4  5\n",
       "26     4.345142     6.657986  4  6\n",
       "27     4.593656     6.932121  4  7\n",
       "28     4.504958     7.370806  4  8\n",
       "29     5.052073     1.268702  5  1\n",
       "30     4.636201     0.793484  5  2\n",
       "31     4.289811     2.065803  5  3\n",
       "32     4.826958     7.039033  5  7\n",
       "33     5.048162     7.044384  5  8\n",
       "34     5.234767     1.016198  6  1\n",
       "35     5.565670     1.458670  6  2\n",
       "36     5.599028     4.048810  6  3\n",
       "37     4.978636     7.155592  6  7\n",
       "38     5.062400     7.141942  6  8"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating dataframe for predicting values comparing with True values\n",
    "yy=pd.DataFrame(yy,columns=['x','y'])\n",
    "y_predd=pd.DataFrame(y_predd, columns=['predicted_x','predicted_y'])\n",
    "df_finall_dnn = pd.DataFrame()\n",
    "df_finall_dnn = pd.concat([y_predd, yy], axis=1)\n",
    "df_finall_dnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22133d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_finall_dnn.to_csv('output_data_dnn.csv', index=False) #for saving output value in csv file for reference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d53520c",
   "metadata": {},
   "source": [
    "**8. Random Forest Regressor for final Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d86c452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 8, 'n_estimators': 50}\n",
      "Mean Squared Error in meter: 0.080\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.283\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 11.03\n",
      "R2 score is in percent: 97.79\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(y_predd,yy)\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(y_predd)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(yy, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,RF_pred)*100))\n",
    "\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, RF_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dae7feb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.44</td>\n",
       "      <td>2.18</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.20</td>\n",
       "      <td>3.18</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.14</td>\n",
       "      <td>3.78</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.24</td>\n",
       "      <td>4.54</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.30</td>\n",
       "      <td>6.32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.36</td>\n",
       "      <td>6.88</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.40</td>\n",
       "      <td>7.94</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.54</td>\n",
       "      <td>1.36</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.04</td>\n",
       "      <td>1.86</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.80</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.68</td>\n",
       "      <td>3.96</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.22</td>\n",
       "      <td>4.90</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.08</td>\n",
       "      <td>6.24</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.70</td>\n",
       "      <td>6.72</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.18</td>\n",
       "      <td>7.82</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.64</td>\n",
       "      <td>1.16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.70</td>\n",
       "      <td>1.66</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.78</td>\n",
       "      <td>2.60</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.16</td>\n",
       "      <td>3.76</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.66</td>\n",
       "      <td>7.86</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.32</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.30</td>\n",
       "      <td>1.72</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.90</td>\n",
       "      <td>3.14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.18</td>\n",
       "      <td>3.74</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.26</td>\n",
       "      <td>4.98</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.08</td>\n",
       "      <td>6.32</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.18</td>\n",
       "      <td>7.12</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.24</td>\n",
       "      <td>7.70</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.24</td>\n",
       "      <td>1.22</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.96</td>\n",
       "      <td>1.74</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.74</td>\n",
       "      <td>3.00</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.12</td>\n",
       "      <td>7.26</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.28</td>\n",
       "      <td>7.84</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.56</td>\n",
       "      <td>1.24</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.76</td>\n",
       "      <td>1.82</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.70</td>\n",
       "      <td>3.38</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.86</td>\n",
       "      <td>7.24</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.82</td>\n",
       "      <td>7.82</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_x  predicted_y  x  y\n",
       "0          1.32         1.48  1  1\n",
       "1          1.44         2.18  1  2\n",
       "2          1.20         3.18  1  3\n",
       "3          1.14         3.78  1  4\n",
       "4          1.24         4.54  1  5\n",
       "5          1.30         6.32  1  6\n",
       "6          1.36         6.88  1  7\n",
       "7          1.40         7.94  1  8\n",
       "8          2.54         1.36  2  1\n",
       "9          2.04         1.86  2  2\n",
       "10         1.80         3.12  2  3\n",
       "11         1.68         3.96  2  4\n",
       "12         2.22         4.90  2  5\n",
       "13         2.08         6.24  2  6\n",
       "14         1.70         6.72  2  7\n",
       "15         2.18         7.82  2  8\n",
       "16         2.64         1.16  3  1\n",
       "17         2.70         1.66  3  2\n",
       "18         2.78         2.60  3  3\n",
       "19         3.16         3.76  3  4\n",
       "20         2.66         7.86  3  8\n",
       "21         4.32         1.48  4  1\n",
       "22         3.30         1.72  4  2\n",
       "23         3.90         3.14  4  3\n",
       "24         4.18         3.74  4  4\n",
       "25         3.26         4.98  4  5\n",
       "26         4.08         6.32  4  6\n",
       "27         4.18         7.12  4  7\n",
       "28         4.24         7.70  4  8\n",
       "29         5.24         1.22  5  1\n",
       "30         4.96         1.74  5  2\n",
       "31         4.74         3.00  5  3\n",
       "32         5.12         7.26  5  7\n",
       "33         5.28         7.84  5  8\n",
       "34         5.56         1.24  6  1\n",
       "35         5.76         1.82  6  2\n",
       "36         5.70         3.38  6  3\n",
       "37         5.86         7.24  6  7\n",
       "38         5.82         7.82  6  8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy=pd.DataFrame(yy,columns=['x','y'])\n",
    "y_predd=pd.DataFrame(RF_pred, columns=['predicted_x','predicted_y'])\n",
    "df_finall = pd.DataFrame()\n",
    "df_finall = pd.concat([y_predd, yy], axis=1)\n",
    "df_finall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3619d084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_finall.to_csv('output_data_with_ex.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d93decd0",
   "metadata": {},
   "source": [
    "**9. KNN Regressor for Final Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1716108a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 0.21\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.46\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 15.95\n",
      "R2 score is in percent: 94.52\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3,5,7,9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(y_predd, yy)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(y_predd)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(yy, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "rmse=sqrt(mean_squared_error(yy, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db91b4fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_x</th>\n",
       "      <th>predicted_y</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.333333</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>6.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3.333333</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>3.666667</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>2.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>7.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>7.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted_x  predicted_y  x  y\n",
       "0      1.333333     1.666667  1  1\n",
       "1      1.333333     1.666667  1  2\n",
       "2      1.333333     3.333333  1  3\n",
       "3      1.333333     3.666667  1  4\n",
       "4      1.333333     4.333333  1  5\n",
       "5      1.333333     6.666667  1  6\n",
       "6      1.333333     6.666667  1  7\n",
       "7      1.333333     7.666667  1  8\n",
       "8      2.666667     1.333333  2  1\n",
       "9      2.000000     2.000000  2  2\n",
       "10     1.666667     3.333333  2  3\n",
       "11     1.333333     4.333333  2  4\n",
       "12     2.333333     5.000000  2  5\n",
       "13     1.666667     6.333333  2  6\n",
       "14     1.333333     6.666667  2  7\n",
       "15     2.000000     8.000000  2  8\n",
       "16     2.666667     1.333333  3  1\n",
       "17     2.666667     1.333333  3  2\n",
       "18     3.333333     2.333333  3  3\n",
       "19     3.666667     3.666667  3  4\n",
       "20     2.000000     8.000000  3  8\n",
       "21     4.666667     1.333333  4  1\n",
       "22     3.000000     1.666667  4  2\n",
       "23     4.333333     3.333333  4  3\n",
       "24     4.333333     3.333333  4  4\n",
       "25     3.000000     4.666667  4  5\n",
       "26     4.000000     7.000000  4  6\n",
       "27     4.000000     7.000000  4  7\n",
       "28     4.333333     7.333333  4  8\n",
       "29     5.333333     1.333333  5  1\n",
       "30     4.666667     1.333333  5  2\n",
       "31     4.333333     3.333333  5  3\n",
       "32     5.333333     7.333333  5  7\n",
       "33     5.333333     7.666667  5  8\n",
       "34     5.666667     1.333333  6  1\n",
       "35     5.666667     1.333333  6  2\n",
       "36     5.666667     2.666667  6  3\n",
       "37     5.666667     7.333333  6  7\n",
       "38     5.666667     7.666667  6  8"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy=pd.DataFrame(yy,columns=['x','y'])\n",
    "y_predd=pd.DataFrame(knn_pred, columns=['predicted_x','predicted_y'])\n",
    "df_finall_knn = pd.DataFrame()\n",
    "df_finall_knn = pd.concat([y_predd, yy], axis=1)\n",
    "df_finall_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a57609e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "XX_train, XX_test, yy_train, yy_test = train_test_split(XX, yy, test_size = 0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c1789e",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9368ec49",
   "metadata": {},
   "source": [
    "**10. Random Forest Regressor only for validate dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "16a27359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 8, 'n_estimators': 100}\n",
      "Mean Squared Error in meter: 0.704\n",
      "Root Mean Squared Error (RMSE) on new data in meter: 0.839\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 29.28\n",
      "R2 score is in percent: 80.82\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from math import sqrt\n",
    "# Define the model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 4, 6, 8]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(XX_train, yy_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Predict on new data with the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "RF_pred = best_model.predict(XX_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(yy_test, RF_pred)\n",
    "print(\"Mean Squared Error in meter: {:.3f}\" .format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(yy_test, RF_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in meter: {:.3f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy_test,RF_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy_test, RF_pred)*100))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46ad4f1f",
   "metadata": {},
   "source": [
    "**11. KNN Regressor only for validate dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b792ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K value found by grid search: 3\n",
      "Mean Squared Error (MSE) on new data in m: 0.90\n",
      "Root Mean Squared Error (RMSE) on new data in m: 0.95\n",
      "Mean Absolute Percentage Error (MAPE) on new data in percentage is : 35.02\n",
      "R2 score is in percent: 76.46\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from math import sqrt\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {'n_neighbors': [3, 5, 7, 9]}\n",
    "\n",
    "# Create a KNN model\n",
    "knn_model = KNeighborsRegressor()\n",
    "\n",
    "# Perform a grid search using cross-validation\n",
    "grid_search = GridSearchCV(knn_model, param_grid, cv=5)\n",
    "grid_search.fit(XX_train,yy_train)\n",
    "\n",
    "# Print the best parameter value found by the grid search\n",
    "print('Best K value found by grid search:', grid_search.best_params_['n_neighbors'])\n",
    "\n",
    "# Get the predictions using the best K value\n",
    "best_knn_model = grid_search.best_estimator_\n",
    "knn_pred = best_knn_model.predict(XX_test)\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "mse = mean_squared_error(yy_test, knn_pred)\n",
    "print('Mean Squared Error (MSE) on new data in m: {:.2f}'.format(mse))\n",
    "\n",
    "rmse=sqrt(mean_squared_error(y_test, knn_pred)) \n",
    "print('Root Mean Squared Error (RMSE) on new data in m: {:.2f}'.format(rmse))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "print('Mean Absolute Percentage Error (MAPE) on new data in percentage is : {:.2f}'.format(mean_absolute_percentage_error(yy_test,knn_pred)*100))\n",
    "\n",
    "print('R2 score is in percent: {:.2f}'.format(r2_score(yy_test, knn_pred)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9318f8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb72dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc2b465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
