Units in the  input layer: 256
Dropout rate in the input layer: 0.3
Activation function: Relu 

Number of hidden layers: 4
Units in the first hidden layer: 112
Activation function in the first hidden layer: relu
Dropout rate in the first hidden layer: 0.4


Units in the second hidden layer: 112
Activation function in the second hidden layer: tanh
Dropout rate in the second hidden layer: 0.3

Units in the third hidden layer: 48
Activation function in the third hidden layer: tanh
Dropout rate in the third hidden layer: 0.3

Units in the fourth hidden layer: 32
Activation function in the fourth hidden layer: relu
Dropout rate in the fourth hidden layer: 0.0


Learning rate: 0.001





